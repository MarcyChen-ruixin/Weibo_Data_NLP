{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-08T22:27:42.684801Z",
     "iopub.status.busy": "2024-08-08T22:27:42.684110Z",
     "iopub.status.idle": "2024-08-08T22:27:43.070396Z",
     "shell.execute_reply": "2024-08-08T22:27:43.069336Z",
     "shell.execute_reply.started": "2024-08-08T22:27:42.684761Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data/data100731\r\n",
      "Archive:  shuju.zip\r\n",
      "  inflating: OCEMOTION.csv           \r\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aistudio/data/data100731/\n",
    "!unzip shuju.zip\n",
    "import pandas as pd\n",
    "data = pd.read_csv('OCEMOTION.csv', sep='\\t',header=None)\n",
    "data.columns = [\"id\", \"text_a\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-08T22:27:44.417707Z",
     "iopub.status.busy": "2024-08-08T22:27:44.416996Z",
     "iopub.status.idle": "2024-08-08T22:27:44.429929Z",
     "shell.execute_reply": "2024-08-08T22:27:44.428873Z",
     "shell.execute_reply.started": "2024-08-08T22:27:44.417663Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def clean_duplication(text):\n",
    "    left_square_brackets_pat = re.compile(r'\\[+')\n",
    "    right_square_brackets_pat = re.compile(r'\\]+')\n",
    "    punct = [',', '\\\\.', '\\\\!', '，', '。', '！', '、', '\\?', '？']\n",
    "\n",
    "    def replace(string, char):\n",
    "        pattern = char + '{2,}'\n",
    "        if char.startswith('\\\\'):\n",
    "            char = char[1:]\n",
    "        string = re.sub(pattern, char, string)\n",
    "        return string\n",
    "\n",
    "    text = left_square_brackets_pat.sub('', text)\n",
    "    text = right_square_brackets_pat.sub('', text)\n",
    "    for p in punct:\n",
    "        text = replace(text, p)\n",
    "    return text\n",
    "\n",
    "def emoji2zh(text, inverse_emoji_dict):\n",
    "    for emoji, ch in inverse_emoji_dict.items():\n",
    "        text = text.replace(emoji, ch)\n",
    "    return text\n",
    "\n",
    "def clean_emotion(data_path, emoji2zh_data, save_dir, train=True):\n",
    "    data = defaultdict(list)\n",
    "    filename = os.path.basename(data_path)\n",
    "    with open(data_path, 'r', encoding='utf8') as f:\n",
    "        texts = f.readlines()\n",
    "        for line in tqdm(texts, desc=data_path):\n",
    "            if train:\n",
    "                id_, text, label = line.strip().split('\\t')\n",
    "            else:\n",
    "                id_, text = line.strip().split('\\t')\n",
    "            data['id'].append(id_)\n",
    "            text = emoji2zh(text, emoji2zh_data)\n",
    "            text = clean_duplication(text)\n",
    "            data['text_a'].append(text)\n",
    "            if train:\n",
    "                data['label'].append(label)\n",
    "    df = pd.DataFrame(data)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    df.to_csv(os.path.join(save_dir, filename), index=False,\n",
    "              encoding='utf8', header=False, sep='\\t')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:27:46.876201Z",
     "iopub.status.busy": "2024-08-08T22:27:46.875552Z",
     "iopub.status.idle": "2024-08-08T22:27:51.680929Z",
     "shell.execute_reply": "2024-08-08T22:27:51.679876Z",
     "shell.execute_reply.started": "2024-08-08T22:27:46.876154Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data/data100731/OCEMOTION.csv: 100%|██████████| 35694/35694 [00:04<00:00, 7710.48it/s]\r\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "emoji2zh_data = json.load(open('/home/aistudio/emoji2zh.json', 'r', encoding='utf8'))\n",
    "data = clean_emotion('/home/aistudio/data/data100731/OCEMOTION.csv',emoji2zh_data,'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:27:53.138330Z",
     "iopub.status.busy": "2024-08-08T22:27:53.137741Z",
     "iopub.status.idle": "2024-08-08T22:27:53.149303Z",
     "shell.execute_reply": "2024-08-08T22:27:53.148242Z",
     "shell.execute_reply.started": "2024-08-08T22:27:53.138294Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data[['text_a', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:27:54.716622Z",
     "iopub.status.busy": "2024-08-08T22:27:54.715967Z",
     "iopub.status.idle": "2024-08-08T22:27:54.747006Z",
     "shell.execute_reply": "2024-08-08T22:27:54.745965Z",
     "shell.execute_reply.started": "2024-08-08T22:27:54.716582Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[data['label']=='sadness', 'label'] = '难过'\n",
    "data.loc[data['label']=='happiness', 'label'] = '愉快'\n",
    "data.loc[data['label']=='like', 'label'] = '喜欢'\n",
    "data.loc[data['label']=='anger', 'label'] = '愤怒'\n",
    "data.loc[data['label']=='fear', 'label'] = '害怕'\n",
    "data.loc[data['label']=='surprise', 'label'] = '惊讶'\n",
    "data.loc[data['label']=='disgust', 'label'] = '厌恶'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:27:56.222424Z",
     "iopub.status.busy": "2024-08-08T22:27:56.221801Z",
     "iopub.status.idle": "2024-08-08T22:27:56.423446Z",
     "shell.execute_reply": "2024-08-08T22:27:56.422460Z",
     "shell.execute_reply.started": "2024-08-08T22:27:56.222378Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集长度： 28558 验证集长度： 3570 测试集长度 3566\r\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train = pd.DataFrame()  \n",
    "valid = pd.DataFrame()  \n",
    "test = pd.DataFrame()  \n",
    "\n",
    "tags = data['label'].unique().tolist() \n",
    "\n",
    "\n",
    "for tag in tags:\n",
    "    target = data[(data['label'] == tag)]\n",
    "    sample = target.sample(int(0.2 * len(target)))\n",
    "    sample_index = sample.index\n",
    "    all_index = target.index\n",
    "    residue_index = all_index.difference(sample_index) \n",
    "    residue = target.loc[residue_index]\n",
    "    test_sample = sample.sample(int(0.5 * len(sample)))\n",
    "    test_sample_index = test_sample.index\n",
    "    valid_sample_index = sample_index.difference(test_sample_index)\n",
    "    valid_sample = sample.loc[valid_sample_index]\n",
    "    test = pd.concat([test, test_sample], ignore_index=True)\n",
    "    valid = pd.concat([valid, valid_sample], ignore_index=True)\n",
    "    train = pd.concat([train, residue], ignore_index=True)\n",
    "    train = shuffle(train)\n",
    "    valid = shuffle(valid)\n",
    "    test = shuffle(test)\n",
    "\n",
    "train.to_csv('train.csv', sep='\\t', index=False) \n",
    "valid.to_csv('valid.csv', sep='\\t', index=False)  \n",
    "test.to_csv('test.csv', sep='\\t', index=False)  \n",
    "\n",
    "print('训练集长度：', len(train), '验证集长度：', len(valid), '测试集长度', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:27:59.298443Z",
     "iopub.status.busy": "2024-08-08T22:27:59.297793Z",
     "iopub.status.idle": "2024-08-08T22:28:20.625485Z",
     "shell.execute_reply": "2024-08-08T22:28:20.624313Z",
     "shell.execute_reply.started": "2024-08-08T22:27:59.298398Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://mirrors.aliyun.com/pypi/simple/\r\n",
      "Requirement already satisfied: paddlehub in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.4)\r\n",
      "Collecting paddlehub\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/4b/40/27c86a86d7697bd503caf84890f50503f86bab4330e848629d6f37625d3f/paddlehub-2.4.0-py3-none-any.whl (213 kB)\r\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/213.9 kB ? eta -:--:--━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/213.9 kB ? eta -:--:--━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/213.9 kB ? eta -:--:--━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/213.9 kB 257.0 kB/s eta 0:00:01━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.7/213.9 kB 264.6 kB/s eta 0:00:01━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/213.9 kB 254.9 kB/s eta 0:00:01━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.2/213.9 kB 259.0 kB/s eta 0:00:01━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.4/213.9 kB 261.2 kB/s eta 0:00:01━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━ 71.7/213.9 kB 260.5 kB/s eta 0:00:01━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━ 81.9/213.9 kB 260.8 kB/s eta 0:00:01━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━ 92.2/213.9 kB 261.3 kB/s eta 0:00:01━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━ 102.4/213.9 kB 261.4 kB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━ 112.6/213.9 kB 261.5 kB/s eta 0:00:01━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━\r\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (5.1.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (21.3)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (8.2.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (3.0.12)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (1.20.3)\r\n",
      "Requirement already satisfied: paddlenlp>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (2.0.7)\r\n",
      "Requirement already satisfied: rarfile in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (3.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (4.66.1)\r\n",
      "Requirement already satisfied: visualdl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (2.2.0)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (0.4.4)\r\n",
      "Requirement already satisfied: flask>=1.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (1.1.1)\r\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (4.1.1.26)\r\n",
      "Requirement already satisfied: gradio in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (3.34.0)\r\n",
      "Collecting paddle2onnx>=0.5.1\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/8d/61/58f5c4bfeefb43eb19e1d883f898ff9baeedb19c597ffe30336323727b5e/paddle2onnx-1.1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.1 MB)\r\n",
      "     l     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.1 MB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.1 MB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.1 MB ? eta -:--:--━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.1 MB 287.1 kB/s eta 0:00:11━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.1 MB 280.5 kB/s eta 0:00:11╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.1 MB 273.3 kB/s eta 0:00:12╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/3.1 MB 273.5 kB/s eta 0:00:12╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/3.1 MB 274.4 kB/s eta 0:00:12╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/3.1 MB 269.6 kB/s eta 0:00:12━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/3.1 MB 268.2 kB/s eta 0:00:12━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/3.1 MB 266.2 kB/s eta 0:00:12━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/3.1 MB 264.4 kB/s eta 0:00:12━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/3.1 MB 264.4 kB/s eta 0:00:12━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.1/3.1 MB 26\r\n",
      "\u001b[?25hRequirement already satisfied: easydict in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (1.9)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (2.2.3)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (4.1.0)\r\n",
      "Requirement already satisfied: gunicorn>=19.10.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (20.0.4)\r\n",
      "Requirement already satisfied: pyzmq in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub) (23.2.1)\r\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub) (0.16.0)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub) (7.0)\r\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub) (3.0.0)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub) (1.1.0)\r\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gunicorn>=19.10.0->paddlehub) (56.2.0)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.42.1)\r\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp>=2.0.0->paddlehub) (1.2.2)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp>=2.0.0->paddlehub) (2.9.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp>=2.0.0->paddlehub) (0.70.11.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (1.1.5)\r\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (0.8.53)\r\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (3.20.1)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (1.16.0)\r\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (1.0.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (2.22.0)\r\n",
      "Requirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (1.21.0)\r\n",
      "Requirement already satisfied: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (0.7.1.1)\r\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0->paddlehub) (4.0.1)\r\n",
      "Requirement already satisfied: gradio-client>=0.2.6 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.2.6)\r\n",
      "Requirement already satisfied: orjson in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (3.9.7)\r\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.3.3)\r\n",
      "Requirement already satisfied: python-multipart in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.0.6)\r\n",
      "Requirement already satisfied: altair>=4.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (4.2.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.16.4)\r\n",
      "Requirement already satisfied: httpx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.24.1)\r\n",
      "Requirement already satisfied: pydub in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.25.1)\r\n",
      "Requirement already satisfied: pydantic in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (1.10.13)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (4.7.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (3.8.6)\r\n",
      "Requirement already satisfied: semantic-version in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (2.10.0)\r\n",
      "Requirement already satisfied: aiofiles in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (23.2.1)\r\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.22.0)\r\n",
      "Requirement already satisfied: websockets>=10.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (11.0.3)\r\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (2.2.0)\r\n",
      "Requirement already satisfied: ffmpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.3.1)\r\n",
      "Requirement already satisfied: fastapi in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (0.85.1)\r\n",
      "Requirement already satisfied: pygments>=2.12.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (2.13.0)\r\n",
      "Requirement already satisfied: markupsafe in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio->paddlehub) (2.0.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlehub) (2.8.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlehub) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlehub) (0.10.0)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlehub) (2019.3)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->paddlehub) (3.0.9)\r\n",
      "Requirement already satisfied: toolz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from altair>=4.2.0->gradio->paddlehub) (0.12.0)\r\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from altair>=4.2.0->gradio->paddlehub) (4.16.0)\r\n",
      "Requirement already satisfied: entrypoints in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from altair>=4.2.0->gradio->paddlehub) (0.4)\r\n",
      "Requirement already satisfied: pycodestyle<2.9.0,>=2.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0->paddlehub) (2.8.0)\r\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0->paddlehub) (0.6.1)\r\n",
      "Requirement already satisfied: importlib-metadata<4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0->paddlehub) (4.2.0)\r\n",
      "Requirement already satisfied: pyflakes<2.5.0,>=2.4.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0->paddlehub) (2.4.0)\r\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0->paddlehub) (2.8.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gradio-client>=0.2.6->gradio->paddlehub) (2023.1.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio->paddlehub) (0.1.1)\r\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio->paddlehub) (2.0.2)\r\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from uvicorn>=0.14.0->gradio->paddlehub) (0.14.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->gradio->paddlehub) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->gradio->paddlehub) (1.3.1)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->gradio->paddlehub) (0.13.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->gradio->paddlehub) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->gradio->paddlehub) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->gradio->paddlehub) (3.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->gradio->paddlehub) (22.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->gradio->paddlehub) (1.9.4)\r\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl>=2.0.0->paddlehub) (3.9.9)\r\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl>=2.0.0->paddlehub) (0.18.0)\r\n",
      "Requirement already satisfied: starlette==0.20.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from fastapi->gradio->paddlehub) (0.20.4)\r\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from starlette==0.20.4->fastapi->gradio->paddlehub) (3.7.1)\r\n",
      "Requirement already satisfied: idna in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from httpx->gradio->paddlehub) (2.8)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from httpx->gradio->paddlehub) (1.3.0)\r\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from httpx->gradio->paddlehub) (0.17.3)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from httpx->gradio->paddlehub) (2019.9.11)\r\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp>=2.0.0->paddlehub) (0.3.3)\r\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0->paddlehub) (16.7.9)\r\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0->paddlehub) (1.4.10)\r\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0->paddlehub) (1.3.0)\r\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0->paddlehub) (1.3.4)\r\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0->paddlehub) (0.10.0)\r\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0->paddlehub) (2.0.1)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0->paddlehub) (1.25.6)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0->paddlehub) (3.0.4)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp>=2.0.0->paddlehub) (0.24.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata<4.3->flake8>=3.7.9->visualdl>=2.0.0->paddlehub) (3.8.1)\r\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->paddlehub) (1.3.10)\r\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->paddlehub) (0.18.1)\r\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->paddlehub) (5.9.0)\r\n",
      "Requirement already satisfied: uc-micro-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->paddlehub) (1.0.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp>=2.0.0->paddlehub) (0.14.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp>=2.0.0->paddlehub) (1.6.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp>=2.0.0->paddlehub) (2.1.0)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi->gradio->paddlehub) (1.2.0)\r\n",
      "Installing collected packages: paddle2onnx, paddlehub\r\n",
      "  Attempting uninstall: paddlehub\r\n",
      "    Found existing installation: paddlehub 2.0.4\r\n",
      "    Uninstalling paddlehub-2.0.4:\r\n",
      "      Successfully uninstalled paddlehub-2.0.4\r\n",
      "Successfully installed paddle2onnx-1.1.0 paddlehub-2.4.0\r\n",
      "\r\n",
      "[notice] A new release of pip available: 22.1.2 -> 24.0\r\n",
      "[notice] To update, run: pip install --upgrade pip\r\n"
     ]
    }
   ],
   "source": [
    "# 下载最新版本的paddlehub\n",
    "!pip install -U paddlehub -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:28:20.653417Z",
     "iopub.status.busy": "2024-08-08T22:28:20.652760Z",
     "iopub.status.idle": "2024-08-08T22:28:20.657563Z",
     "shell.execute_reply": "2024-08-08T22:28:20.656712Z",
     "shell.execute_reply.started": "2024-08-08T22:28:20.653378Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddlehub as hub\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:28:39.130898Z",
     "iopub.status.busy": "2024-08-08T22:28:39.130240Z",
     "iopub.status.idle": "2024-08-08T22:28:39.141509Z",
     "shell.execute_reply": "2024-08-08T22:28:39.140685Z",
     "shell.execute_reply.started": "2024-08-08T22:28:39.130850Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['难过', '愉快', '喜欢', '愤怒', '害怕', '惊讶', '厌恶']\r\n",
      "{0: '难过', 1: '愉快', 2: '喜欢', 3: '愤怒', 4: '害怕', 5: '惊讶', 6: '厌恶'}\r\n"
     ]
    }
   ],
   "source": [
    "label_list=list(data.label.unique())\n",
    "print(label_list)\n",
    "\n",
    "label_map = {\n",
    "    idx: label_text for idx, label_text in enumerate(label_list)\n",
    "}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:28:40.697596Z",
     "iopub.status.busy": "2024-08-08T22:28:40.696952Z",
     "iopub.status.idle": "2024-08-08T22:29:17.267564Z",
     "shell.execute_reply": "2024-08-08T22:29:17.266560Z",
     "shell.execute_reply.started": "2024-08-08T22:28:40.697552Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download https://bj.bcebos.com/paddlehub/paddlehub_dev/ernie_tiny.zip\r\n",
      "[##################################################] 100.00%\r\n",
      "Decompress /home/aistudio/.paddlehub/tmp/tmp6xtb3780/ernie_tiny.zip\r\n",
      "[##################################################] 100.00%\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-09 06:28:41,378] [    INFO] - Successfully installed ernie_tiny-2.0.2\r\n",
      "[2024-08-09 06:28:41,383] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie_tiny/ernie_tiny.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-tiny\r\n",
      "[2024-08-09 06:28:41,386] [    INFO] - Downloading ernie_tiny.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_tiny/ernie_tiny.pdparams\r\n",
      "100%|██████████| 354158/354158 [00:23<00:00, 15348.31it/s]\r\n",
      "W0809 06:29:04.635701   277 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 10.1\r\n",
      "W0809 06:29:04.642895   277 device_context.cc:422] device: 0, cuDNN Version: 7.6.\r\n"
     ]
    }
   ],
   "source": [
    "model = hub.Module(name=\"ernie_tiny\", task='seq-cls', num_classes=7, label_map=label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:29:17.289626Z",
     "iopub.status.busy": "2024-08-08T22:29:17.289223Z",
     "iopub.status.idle": "2024-08-08T22:29:17.298621Z",
     "shell.execute_reply": "2024-08-08T22:29:17.297954Z",
     "shell.execute_reply.started": "2024-08-08T22:29:17.289601Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, io, csv\n",
    "from paddlehub.datasets.base_nlp_dataset import InputExample, TextClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:29:17.300656Z",
     "iopub.status.busy": "2024-08-08T22:29:17.300163Z",
     "iopub.status.idle": "2024-08-08T22:29:18.557669Z",
     "shell.execute_reply": "2024-08-08T22:29:18.556640Z",
     "shell.execute_reply.started": "2024-08-08T22:29:17.300631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR=\"/home/aistudio/data/data100731/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:29:18.559373Z",
     "iopub.status.busy": "2024-08-08T22:29:18.558946Z",
     "iopub.status.idle": "2024-08-08T22:29:49.437068Z",
     "shell.execute_reply": "2024-08-08T22:29:49.436144Z",
     "shell.execute_reply.started": "2024-08-08T22:29:18.559345Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-09 06:29:18,994] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_tiny/vocab.txt\r\n",
      "100%|██████████| 459/459 [00:00<00:00, 12935.12it/s]\r\n",
      "[2024-08-09 06:29:19,159] [    INFO] - Downloading spm_cased_simp_sampled.model from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_tiny/spm_cased_simp_sampled.model\r\n",
      "100%|██████████| 1083/1083 [00:00<00:00, 32310.46it/s]\r\n",
      "[2024-08-09 06:29:19,322] [    INFO] - Downloading dict.wordseg.pickle from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_tiny/dict.wordseg.pickle\r\n",
      "100%|██████████| 161822/161822 [00:02<00:00, 64249.21it/s]\r\n",
      "[2024-08-09 06:29:35,847] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/vocab.txt\r\n",
      "[2024-08-09 06:29:35,850] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/spm_cased_simp_sampled.model\r\n",
      "[2024-08-09 06:29:35,853] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/dict.wordseg.pickle\r\n",
      "[2024-08-09 06:29:42,556] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/vocab.txt\r\n",
      "[2024-08-09 06:29:42,560] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/spm_cased_simp_sampled.model\r\n",
      "[2024-08-09 06:29:42,564] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/dict.wordseg.pickle\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=有时候很脆弱大风已吹成狗\tlabel=难过\r\n",
      "text=我的梦想就是组成一个绿担小分队(至少5个人),打扮成女子偶像的样子(以绿色为主),然后一起去看拉西的con,在台下跳完全场。但是现在我连一个能和我去的姑娘都找不到。\tlabel=难过\r\n",
      "text=繁华三千,看淡那即是浮云;烦恼无数,等你想开了也就是晴天\tlabel=愉快\r\n",
      "text=在看涉外大饭店,里面一个老太太摔断了髋骨,医生说要换新的但是要等六个月,老太太说六个月?你知不知道我这么大岁数的人连绿香蕉都不敢买?!\tlabel=愉快\r\n",
      "text=心静了才能听见自己的内心也许,你正狂热追求的,并非你真正想要的,只是迁就了别人或者社会;也许,你正为之苦恼,肝肠寸断的,未必是你真正想爱或者该爱的,只是一时的不甘心。坐下来,静赏花开,静观水流,心静了,自然就看清了。笑看花开,是一种宁静的喜悦;静赏花落,是一份随缘的自在\tlabel=愉快\r\n",
      "text=打个电话能知道这么多的事情真的是越来越恶心你们了我以前就想不通为什么这么多人说你们不好好吧我现在知道了求求你们闭上你们的嘴收起你们那一副烂好人的样子不是每个人都会给你们发好人卡。不是你们的事情不要多说不要多问关你逼事\tlabel=厌恶\r\n",
      "text=哈哈。关注与粉丝都低于100啦!鼓掌。\tlabel=喜欢\r\n",
      "text=不知为啥,看与听得越多,我就越觉得那小子好玩,真的好可爱。除了不觉值得我依靠之外,呵呵。是我放不下忘不了他吧,\tlabel=难过\r\n",
      "text=〖回家第一晚〗温暖,亲切,自由自在的享受轻松的生活哈哈哈哈哈哈\tlabel=愉快\r\n"
     ]
    }
   ],
   "source": [
    "class OCEMOTION(TextClassificationDataset):\n",
    "    def __init__(self, tokenizer, mode='train', max_seq_len=128):\n",
    "        if mode == 'train':\n",
    "            data_file = 'train.csv'  \n",
    "        elif mode == 'test':\n",
    "            data_file = 'test.csv'   \n",
    "        else:\n",
    "            data_file = 'valid.csv' \n",
    "\n",
    "        super(OCEMOTION, self).__init__(\n",
    "            base_path=DATA_DIR,\n",
    "            data_file=data_file,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=max_seq_len,\n",
    "            mode=mode,\n",
    "            is_file_with_header=True,\n",
    "            label_list=label_list\n",
    "            )\n",
    "\n",
    "    # 解析文本文件里的样本\n",
    "    def _read_file(self, input_file, is_file_with_header: bool = False):\n",
    "        if not os.path.exists(input_file):\n",
    "            raise RuntimeError(\"The file {} is not found.\".format(input_file))\n",
    "        else:\n",
    "            with io.open(input_file, \"r\", encoding=\"UTF-8\") as f:\n",
    "                reader = csv.reader(f, delimiter=\"\\t\")\n",
    "                examples = []\n",
    "                seq_id = 0\n",
    "                header = next(reader) if is_file_with_header else None\n",
    "                for line in reader:\n",
    "                    try:\n",
    "                        example = InputExample(guid=seq_id, text_a=line[0], label=line[1])\n",
    "                        seq_id += 1\n",
    "                        examples.append(example)\n",
    "                    except:\n",
    "                        continue\n",
    "                return examples\n",
    "\n",
    "train_dataset = OCEMOTION(model.get_tokenizer(), mode='train', max_seq_len=128)  # max_seq_len根据具体文本长度进行确定，但需注意max_seq_len最长不超过512\n",
    "dev_dataset = OCEMOTION(model.get_tokenizer(), mode='dev', max_seq_len=128)\n",
    "test_dataset = OCEMOTION(model.get_tokenizer(), mode='test', max_seq_len=128)\n",
    "\n",
    "for e in train_dataset.examples[:3]:\n",
    "    print(e)\n",
    "for e in dev_dataset.examples[:3]:\n",
    "    print(e)\n",
    "for e in test_dataset.examples[:3]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:29:49.438567Z",
     "iopub.status.busy": "2024-08-08T22:29:49.438185Z",
     "iopub.status.idle": "2024-08-08T22:29:49.443097Z",
     "shell.execute_reply": "2024-08-08T22:29:49.442332Z",
     "shell.execute_reply.started": "2024-08-08T22:29:49.438540Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:30:48.743127Z",
     "iopub.status.busy": "2024-08-08T22:30:48.742505Z",
     "iopub.status.idle": "2024-08-08T22:30:48.751632Z",
     "shell.execute_reply": "2024-08-08T22:30:48.750928Z",
     "shell.execute_reply.started": "2024-08-08T22:30:48.743087Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-09 06:30:48,746] [ WARNING] - PaddleHub model checkpoint not found, start from scratch...\r\n"
     ]
    }
   ],
   "source": [
    "trainer = hub.Trainer(model, optimizer, checkpoint_dir='./ckpt', use_gpu=True, use_vdl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:31:13.804549Z",
     "iopub.status.busy": "2024-08-08T22:31:13.803874Z",
     "iopub.status.idle": "2024-08-08T22:45:36.110131Z",
     "shell.execute_reply": "2024-08-08T22:45:36.109167Z",
     "shell.execute_reply.started": "2024-08-08T22:31:13.804503Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-09 06:31:15,304] [   TRAIN] - Epoch=1/4, Step=10/1785 loss=1.8942 acc=0.2250 lr=0.000020 step/sec=6.67 | ETA 00:17:49\r\n",
      "[2024-08-09 06:31:16,425] [   TRAIN] - Epoch=1/4, Step=20/1785 loss=1.6162 acc=0.4000 lr=0.000020 step/sec=8.93 | ETA 00:15:34\r\n",
      "[2024-08-09 06:31:17,577] [   TRAIN] - Epoch=1/4, Step=30/1785 loss=1.5881 acc=0.3750 lr=0.000020 step/sec=8.68 | ETA 00:14:57\r\n",
      "[2024-08-09 06:31:18,780] [   TRAIN] - Epoch=1/4, Step=40/1785 loss=1.5635 acc=0.4000 lr=0.000020 step/sec=8.31 | ETA 00:14:47\r\n",
      "[2024-08-09 06:31:19,901] [   TRAIN] - Epoch=1/4, Step=50/1785 loss=1.6467 acc=0.3438 lr=0.000020 step/sec=8.92 | ETA 00:14:30\r\n",
      "[2024-08-09 06:31:21,092] [   TRAIN] - Epoch=1/4, Step=60/1785 loss=1.4888 acc=0.4313 lr=0.000020 step/sec=8.40 | ETA 00:14:27\r\n",
      "[2024-08-09 06:31:22,221] [   TRAIN] - Epoch=1/4, Step=70/1785 loss=1.5408 acc=0.4375 lr=0.000020 step/sec=8.86 | ETA 00:14:18\r\n",
      "[2024-08-09 06:31:23,447] [   TRAIN] - Epoch=1/4, Step=80/1785 loss=1.4238 acc=0.5125 lr=0.000020 step/sec=8.15 | ETA 00:14:20\r\n",
      "[2024-08-09 06:31:24,639] [   TRAIN] - Epoch=1/4, Step=90/1785 loss=1.5755 acc=0.4062 lr=0.000020 step/sec=8.39 | ETA 00:14:19\r\n",
      "[2024-08-09 06:31:25,869] [   TRAIN] - Epoch=1/4, Step=100/1785 loss=1.3754 acc=0.5062 lr=0.000020 step/sec=8.13 | ETA 00:14:21\r\n",
      "[2024-08-09 06:31:27,090] [   TRAIN] - Epoch=1/4, Step=110/1785 loss=1.3867 acc=0.5312 lr=0.000020 step/sec=8.19 | ETA 00:14:22\r\n",
      "[2024-08-09 06:31:28,220] [   TRAIN] - Epoch=1/4, Step=120/1785 loss=1.4403 acc=0.4750 lr=0.000020 step/sec=8.85 | ETA 00:14:17\r\n",
      "[2024-08-09 06:31:29,352] [   TRAIN] - Epoch=1/4, Step=130/1785 loss=1.3951 acc=0.4938 lr=0.000020 step/sec=8.84 | ETA 00:14:13\r\n",
      "[2024-08-09 06:31:30,582] [   TRAIN] - Epoch=1/4, Step=140/1785 loss=1.3758 acc=0.5125 lr=0.000020 step/sec=8.13 | ETA 00:14:15\r\n",
      "[2024-08-09 06:31:31,678] [   TRAIN] - Epoch=1/4, Step=150/1785 loss=1.3982 acc=0.4938 lr=0.000020 step/sec=9.12 | ETA 00:14:10\r\n",
      "[2024-08-09 06:31:32,861] [   TRAIN] - Epoch=1/4, Step=160/1785 loss=1.2863 acc=0.5312 lr=0.000020 step/sec=8.46 | ETA 00:14:10\r\n",
      "[2024-08-09 06:31:33,933] [   TRAIN] - Epoch=1/4, Step=170/1785 loss=1.3054 acc=0.5687 lr=0.000020 step/sec=9.32 | ETA 00:14:05\r\n",
      "[2024-08-09 06:31:35,106] [   TRAIN] - Epoch=1/4, Step=180/1785 loss=1.3405 acc=0.5062 lr=0.000020 step/sec=8.53 | ETA 00:14:04\r\n",
      "[2024-08-09 06:31:36,171] [   TRAIN] - Epoch=1/4, Step=190/1785 loss=1.2523 acc=0.5500 lr=0.000020 step/sec=9.39 | ETA 00:14:00\r\n",
      "[2024-08-09 06:31:37,334] [   TRAIN] - Epoch=1/4, Step=200/1785 loss=1.3729 acc=0.4813 lr=0.000020 step/sec=8.60 | ETA 00:13:59\r\n",
      "[2024-08-09 06:31:38,435] [   TRAIN] - Epoch=1/4, Step=210/1785 loss=1.2447 acc=0.5437 lr=0.000020 step/sec=9.08 | ETA 00:13:57\r\n",
      "[2024-08-09 06:31:39,525] [   TRAIN] - Epoch=1/4, Step=220/1785 loss=1.3731 acc=0.5062 lr=0.000020 step/sec=9.18 | ETA 00:13:54\r\n",
      "[2024-08-09 06:31:40,671] [   TRAIN] - Epoch=1/4, Step=230/1785 loss=1.4396 acc=0.4813 lr=0.000020 step/sec=8.73 | ETA 00:13:53\r\n",
      "[2024-08-09 06:31:41,776] [   TRAIN] - Epoch=1/4, Step=240/1785 loss=1.1976 acc=0.5437 lr=0.000020 step/sec=9.05 | ETA 00:13:52\r\n",
      "[2024-08-09 06:31:42,874] [   TRAIN] - Epoch=1/4, Step=250/1785 loss=1.3191 acc=0.5250 lr=0.000020 step/sec=9.10 | ETA 00:13:50\r\n",
      "[2024-08-09 06:31:43,986] [   TRAIN] - Epoch=1/4, Step=260/1785 loss=1.3115 acc=0.5437 lr=0.000020 step/sec=9.00 | ETA 00:13:48\r\n",
      "[2024-08-09 06:31:45,163] [   TRAIN] - Epoch=1/4, Step=270/1785 loss=1.1789 acc=0.6000 lr=0.000020 step/sec=8.49 | ETA 00:13:49\r\n",
      "[2024-08-09 06:31:46,247] [   TRAIN] - Epoch=1/4, Step=280/1785 loss=1.2330 acc=0.5687 lr=0.000020 step/sec=9.22 | ETA 00:13:47\r\n",
      "[2024-08-09 06:31:47,317] [   TRAIN] - Epoch=1/4, Step=290/1785 loss=1.3831 acc=0.5000 lr=0.000020 step/sec=9.34 | ETA 00:13:45\r\n",
      "[2024-08-09 06:31:48,433] [   TRAIN] - Epoch=1/4, Step=300/1785 loss=1.2758 acc=0.5500 lr=0.000020 step/sec=8.96 | ETA 00:13:44\r\n",
      "[2024-08-09 06:31:49,528] [   TRAIN] - Epoch=1/4, Step=310/1785 loss=1.2635 acc=0.5312 lr=0.000020 step/sec=9.13 | ETA 00:13:42\r\n",
      "[2024-08-09 06:31:50,704] [   TRAIN] - Epoch=1/4, Step=320/1785 loss=1.3411 acc=0.5125 lr=0.000020 step/sec=8.51 | ETA 00:13:43\r\n",
      "[2024-08-09 06:31:51,774] [   TRAIN] - Epoch=1/4, Step=330/1785 loss=1.1919 acc=0.6000 lr=0.000020 step/sec=9.34 | ETA 00:13:41\r\n",
      "[2024-08-09 06:31:52,953] [   TRAIN] - Epoch=1/4, Step=340/1785 loss=1.2665 acc=0.5375 lr=0.000020 step/sec=8.48 | ETA 00:13:42\r\n",
      "[2024-08-09 06:31:54,023] [   TRAIN] - Epoch=1/4, Step=350/1785 loss=1.2633 acc=0.5188 lr=0.000020 step/sec=9.35 | ETA 00:13:40\r\n",
      "[2024-08-09 06:31:55,145] [   TRAIN] - Epoch=1/4, Step=360/1785 loss=1.2279 acc=0.5188 lr=0.000020 step/sec=8.91 | ETA 00:13:39\r\n",
      "[2024-08-09 06:31:56,305] [   TRAIN] - Epoch=1/4, Step=370/1785 loss=1.2371 acc=0.5062 lr=0.000020 step/sec=8.62 | ETA 00:13:40\r\n",
      "[2024-08-09 06:31:57,464] [   TRAIN] - Epoch=1/4, Step=380/1785 loss=1.1521 acc=0.5938 lr=0.000020 step/sec=8.63 | ETA 00:13:40\r\n",
      "[2024-08-09 06:31:58,589] [   TRAIN] - Epoch=1/4, Step=390/1785 loss=1.3277 acc=0.5500 lr=0.000020 step/sec=8.89 | ETA 00:13:39\r\n",
      "[2024-08-09 06:31:59,778] [   TRAIN] - Epoch=1/4, Step=400/1785 loss=1.0356 acc=0.6687 lr=0.000020 step/sec=8.42 | ETA 00:13:40\r\n",
      "[2024-08-09 06:32:00,831] [   TRAIN] - Epoch=1/4, Step=410/1785 loss=1.2719 acc=0.4875 lr=0.000020 step/sec=9.49 | ETA 00:13:38\r\n",
      "[2024-08-09 06:32:01,982] [   TRAIN] - Epoch=1/4, Step=420/1785 loss=1.1566 acc=0.5563 lr=0.000020 step/sec=8.69 | ETA 00:13:38\r\n",
      "[2024-08-09 06:32:03,155] [   TRAIN] - Epoch=1/4, Step=430/1785 loss=1.0669 acc=0.6000 lr=0.000020 step/sec=8.53 | ETA 00:13:39\r\n",
      "[2024-08-09 06:32:04,189] [   TRAIN] - Epoch=1/4, Step=440/1785 loss=1.0701 acc=0.6250 lr=0.000020 step/sec=9.67 | ETA 00:13:37\r\n",
      "[2024-08-09 06:32:05,352] [   TRAIN] - Epoch=1/4, Step=450/1785 loss=1.2879 acc=0.5125 lr=0.000020 step/sec=8.59 | ETA 00:13:37\r\n",
      "[2024-08-09 06:32:06,517] [   TRAIN] - Epoch=1/4, Step=460/1785 loss=1.2762 acc=0.5375 lr=0.000020 step/sec=8.59 | ETA 00:13:38\r\n",
      "[2024-08-09 06:32:07,705] [   TRAIN] - Epoch=1/4, Step=470/1785 loss=1.3713 acc=0.5125 lr=0.000020 step/sec=8.42 | ETA 00:13:38\r\n",
      "[2024-08-09 06:32:08,846] [   TRAIN] - Epoch=1/4, Step=480/1785 loss=1.2130 acc=0.5437 lr=0.000020 step/sec=8.76 | ETA 00:13:38\r\n",
      "[2024-08-09 06:32:09,943] [   TRAIN] - Epoch=1/4, Step=490/1785 loss=1.1823 acc=0.5563 lr=0.000020 step/sec=9.11 | ETA 00:13:38\r\n",
      "[2024-08-09 06:32:11,033] [   TRAIN] - Epoch=1/4, Step=500/1785 loss=1.3069 acc=0.5250 lr=0.000020 step/sec=9.18 | ETA 00:13:37\r\n",
      "[2024-08-09 06:32:12,250] [   TRAIN] - Epoch=1/4, Step=510/1785 loss=1.1145 acc=0.5687 lr=0.000020 step/sec=8.22 | ETA 00:13:38\r\n",
      "[2024-08-09 06:32:13,369] [   TRAIN] - Epoch=1/4, Step=520/1785 loss=1.1647 acc=0.5750 lr=0.000020 step/sec=8.94 | ETA 00:13:37\r\n",
      "[2024-08-09 06:32:14,460] [   TRAIN] - Epoch=1/4, Step=530/1785 loss=1.1327 acc=0.5563 lr=0.000020 step/sec=9.16 | ETA 00:13:37\r\n",
      "[2024-08-09 06:32:15,566] [   TRAIN] - Epoch=1/4, Step=540/1785 loss=1.2417 acc=0.5250 lr=0.000020 step/sec=9.04 | ETA 00:13:36\r\n",
      "[2024-08-09 06:32:16,675] [   TRAIN] - Epoch=1/4, Step=550/1785 loss=1.1455 acc=0.5750 lr=0.000020 step/sec=9.02 | ETA 00:13:36\r\n",
      "[2024-08-09 06:32:17,761] [   TRAIN] - Epoch=1/4, Step=560/1785 loss=1.1023 acc=0.6250 lr=0.000020 step/sec=9.21 | ETA 00:13:35\r\n",
      "[2024-08-09 06:32:18,897] [   TRAIN] - Epoch=1/4, Step=570/1785 loss=1.0413 acc=0.5938 lr=0.000020 step/sec=8.80 | ETA 00:13:35\r\n",
      "[2024-08-09 06:32:20,094] [   TRAIN] - Epoch=1/4, Step=580/1785 loss=1.0438 acc=0.6375 lr=0.000020 step/sec=8.35 | ETA 00:13:36\r\n",
      "[2024-08-09 06:32:21,191] [   TRAIN] - Epoch=1/4, Step=590/1785 loss=1.2836 acc=0.5188 lr=0.000020 step/sec=9.12 | ETA 00:13:35\r\n",
      "[2024-08-09 06:32:22,275] [   TRAIN] - Epoch=1/4, Step=600/1785 loss=1.3172 acc=0.5500 lr=0.000020 step/sec=9.22 | ETA 00:13:34\r\n",
      "[2024-08-09 06:32:23,373] [   TRAIN] - Epoch=1/4, Step=610/1785 loss=1.2662 acc=0.5625 lr=0.000020 step/sec=9.11 | ETA 00:13:34\r\n",
      "[2024-08-09 06:32:24,452] [   TRAIN] - Epoch=1/4, Step=620/1785 loss=1.2279 acc=0.5563 lr=0.000020 step/sec=9.27 | ETA 00:13:33\r\n",
      "[2024-08-09 06:32:25,537] [   TRAIN] - Epoch=1/4, Step=630/1785 loss=1.1698 acc=0.5312 lr=0.000020 step/sec=9.21 | ETA 00:13:32\r\n",
      "[2024-08-09 06:32:26,744] [   TRAIN] - Epoch=1/4, Step=640/1785 loss=1.2027 acc=0.5563 lr=0.000020 step/sec=8.29 | ETA 00:13:33\r\n",
      "[2024-08-09 06:32:27,827] [   TRAIN] - Epoch=1/4, Step=650/1785 loss=1.1356 acc=0.6188 lr=0.000020 step/sec=9.23 | ETA 00:13:33\r\n",
      "[2024-08-09 06:32:28,918] [   TRAIN] - Epoch=1/4, Step=660/1785 loss=1.1607 acc=0.6188 lr=0.000020 step/sec=9.17 | ETA 00:13:32\r\n",
      "[2024-08-09 06:32:30,105] [   TRAIN] - Epoch=1/4, Step=670/1785 loss=1.1608 acc=0.5750 lr=0.000020 step/sec=8.43 | ETA 00:13:33\r\n",
      "[2024-08-09 06:32:31,243] [   TRAIN] - Epoch=1/4, Step=680/1785 loss=1.1404 acc=0.5813 lr=0.000020 step/sec=8.78 | ETA 00:13:33\r\n",
      "[2024-08-09 06:32:32,376] [   TRAIN] - Epoch=1/4, Step=690/1785 loss=1.3722 acc=0.5312 lr=0.000020 step/sec=8.83 | ETA 00:13:33\r\n",
      "[2024-08-09 06:32:33,488] [   TRAIN] - Epoch=1/4, Step=700/1785 loss=1.2088 acc=0.5813 lr=0.000020 step/sec=8.99 | ETA 00:13:32\r\n",
      "[2024-08-09 06:32:34,605] [   TRAIN] - Epoch=1/4, Step=710/1785 loss=1.1204 acc=0.5813 lr=0.000020 step/sec=8.96 | ETA 00:13:32\r\n",
      "[2024-08-09 06:32:35,730] [   TRAIN] - Epoch=1/4, Step=720/1785 loss=1.3461 acc=0.4938 lr=0.000020 step/sec=8.89 | ETA 00:13:32\r\n",
      "[2024-08-09 06:32:36,863] [   TRAIN] - Epoch=1/4, Step=730/1785 loss=1.1365 acc=0.5875 lr=0.000020 step/sec=8.82 | ETA 00:13:32\r\n",
      "[2024-08-09 06:32:37,959] [   TRAIN] - Epoch=1/4, Step=740/1785 loss=1.1824 acc=0.5500 lr=0.000020 step/sec=9.12 | ETA 00:13:31\r\n",
      "[2024-08-09 06:32:39,049] [   TRAIN] - Epoch=1/4, Step=750/1785 loss=1.1208 acc=0.5813 lr=0.000020 step/sec=9.17 | ETA 00:13:31\r\n",
      "[2024-08-09 06:32:40,160] [   TRAIN] - Epoch=1/4, Step=760/1785 loss=1.2510 acc=0.5188 lr=0.000020 step/sec=9.00 | ETA 00:13:31\r\n",
      "[2024-08-09 06:32:41,371] [   TRAIN] - Epoch=1/4, Step=770/1785 loss=1.0800 acc=0.6250 lr=0.000020 step/sec=8.26 | ETA 00:13:31\r\n",
      "[2024-08-09 06:32:42,456] [   TRAIN] - Epoch=1/4, Step=780/1785 loss=1.1717 acc=0.5500 lr=0.000020 step/sec=9.22 | ETA 00:13:31\r\n",
      "[2024-08-09 06:32:43,551] [   TRAIN] - Epoch=1/4, Step=790/1785 loss=1.3278 acc=0.4938 lr=0.000020 step/sec=9.13 | ETA 00:13:31\r\n",
      "[2024-08-09 06:32:44,574] [   TRAIN] - Epoch=1/4, Step=800/1785 loss=1.1976 acc=0.5312 lr=0.000020 step/sec=9.78 | ETA 00:13:30\r\n",
      "[2024-08-09 06:32:45,726] [   TRAIN] - Epoch=1/4, Step=810/1785 loss=1.1207 acc=0.5750 lr=0.000020 step/sec=8.68 | ETA 00:13:30\r\n",
      "[2024-08-09 06:32:46,866] [   TRAIN] - Epoch=1/4, Step=820/1785 loss=1.1916 acc=0.5375 lr=0.000020 step/sec=8.77 | ETA 00:13:30\r\n",
      "[2024-08-09 06:32:47,975] [   TRAIN] - Epoch=1/4, Step=830/1785 loss=0.9983 acc=0.6687 lr=0.000020 step/sec=9.01 | ETA 00:13:30\r\n",
      "[2024-08-09 06:32:49,037] [   TRAIN] - Epoch=1/4, Step=840/1785 loss=1.2407 acc=0.5188 lr=0.000020 step/sec=9.42 | ETA 00:13:29\r\n",
      "[2024-08-09 06:32:50,154] [   TRAIN] - Epoch=1/4, Step=850/1785 loss=1.1243 acc=0.5750 lr=0.000020 step/sec=8.95 | ETA 00:13:29\r\n",
      "[2024-08-09 06:32:51,323] [   TRAIN] - Epoch=1/4, Step=860/1785 loss=1.2884 acc=0.5312 lr=0.000020 step/sec=8.55 | ETA 00:13:29\r\n",
      "[2024-08-09 06:32:52,382] [   TRAIN] - Epoch=1/4, Step=870/1785 loss=1.1038 acc=0.5938 lr=0.000020 step/sec=9.45 | ETA 00:13:29\r\n",
      "[2024-08-09 06:32:53,495] [   TRAIN] - Epoch=1/4, Step=880/1785 loss=1.4138 acc=0.5125 lr=0.000020 step/sec=8.99 | ETA 00:13:28\r\n",
      "[2024-08-09 06:32:54,558] [   TRAIN] - Epoch=1/4, Step=890/1785 loss=1.1403 acc=0.5875 lr=0.000020 step/sec=9.41 | ETA 00:13:28\r\n",
      "[2024-08-09 06:32:55,794] [   TRAIN] - Epoch=1/4, Step=900/1785 loss=1.1839 acc=0.5437 lr=0.000020 step/sec=8.09 | ETA 00:13:29\r\n",
      "[2024-08-09 06:32:56,924] [   TRAIN] - Epoch=1/4, Step=910/1785 loss=1.1268 acc=0.5813 lr=0.000020 step/sec=8.85 | ETA 00:13:29\r\n",
      "[2024-08-09 06:32:58,020] [   TRAIN] - Epoch=1/4, Step=920/1785 loss=1.2199 acc=0.5125 lr=0.000020 step/sec=9.12 | ETA 00:13:28\r\n",
      "[2024-08-09 06:32:59,114] [   TRAIN] - Epoch=1/4, Step=930/1785 loss=1.0563 acc=0.6562 lr=0.000020 step/sec=9.15 | ETA 00:13:28\r\n",
      "[2024-08-09 06:33:00,210] [   TRAIN] - Epoch=1/4, Step=940/1785 loss=1.0594 acc=0.6125 lr=0.000020 step/sec=9.12 | ETA 00:13:28\r\n",
      "[2024-08-09 06:33:01,300] [   TRAIN] - Epoch=1/4, Step=950/1785 loss=1.1518 acc=0.5563 lr=0.000020 step/sec=9.18 | ETA 00:13:27\r\n",
      "[2024-08-09 06:33:02,510] [   TRAIN] - Epoch=1/4, Step=960/1785 loss=1.0465 acc=0.6125 lr=0.000020 step/sec=8.26 | ETA 00:13:28\r\n",
      "[2024-08-09 06:33:03,622] [   TRAIN] - Epoch=1/4, Step=970/1785 loss=1.3151 acc=0.5000 lr=0.000020 step/sec=8.99 | ETA 00:13:28\r\n",
      "[2024-08-09 06:33:04,706] [   TRAIN] - Epoch=1/4, Step=980/1785 loss=1.1864 acc=0.5687 lr=0.000020 step/sec=9.23 | ETA 00:13:27\r\n",
      "[2024-08-09 06:33:05,802] [   TRAIN] - Epoch=1/4, Step=990/1785 loss=1.0525 acc=0.6188 lr=0.000020 step/sec=9.12 | ETA 00:13:27\r\n",
      "[2024-08-09 06:33:06,898] [   TRAIN] - Epoch=1/4, Step=1000/1785 loss=1.0547 acc=0.6375 lr=0.000020 step/sec=9.12 | ETA 00:13:27\r\n",
      "[2024-08-09 06:33:08,000] [   TRAIN] - Epoch=1/4, Step=1010/1785 loss=1.0406 acc=0.6312 lr=0.000020 step/sec=9.08 | ETA 00:13:27\r\n",
      "[2024-08-09 06:33:09,089] [   TRAIN] - Epoch=1/4, Step=1020/1785 loss=1.2074 acc=0.5250 lr=0.000020 step/sec=9.18 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:10,195] [   TRAIN] - Epoch=1/4, Step=1030/1785 loss=1.2721 acc=0.5062 lr=0.000020 step/sec=9.04 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:11,384] [   TRAIN] - Epoch=1/4, Step=1040/1785 loss=1.1406 acc=0.5938 lr=0.000020 step/sec=8.41 | ETA 00:13:27\r\n",
      "[2024-08-09 06:33:12,463] [   TRAIN] - Epoch=1/4, Step=1050/1785 loss=1.1398 acc=0.5437 lr=0.000020 step/sec=9.27 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:13,567] [   TRAIN] - Epoch=1/4, Step=1060/1785 loss=1.0500 acc=0.6875 lr=0.000020 step/sec=9.06 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:14,672] [   TRAIN] - Epoch=1/4, Step=1070/1785 loss=1.2417 acc=0.5687 lr=0.000020 step/sec=9.05 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:15,735] [   TRAIN] - Epoch=1/4, Step=1080/1785 loss=1.1419 acc=0.5750 lr=0.000020 step/sec=9.41 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:16,867] [   TRAIN] - Epoch=1/4, Step=1090/1785 loss=1.2848 acc=0.5312 lr=0.000020 step/sec=8.84 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:18,013] [   TRAIN] - Epoch=1/4, Step=1100/1785 loss=1.0629 acc=0.6062 lr=0.000020 step/sec=8.72 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:19,058] [   TRAIN] - Epoch=1/4, Step=1110/1785 loss=1.1940 acc=0.5437 lr=0.000020 step/sec=9.57 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:20,180] [   TRAIN] - Epoch=1/4, Step=1120/1785 loss=1.2419 acc=0.5437 lr=0.000020 step/sec=8.92 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:21,276] [   TRAIN] - Epoch=1/4, Step=1130/1785 loss=1.2340 acc=0.5750 lr=0.000020 step/sec=9.12 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:22,493] [   TRAIN] - Epoch=1/4, Step=1140/1785 loss=1.1129 acc=0.6000 lr=0.000020 step/sec=8.22 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:23,696] [   TRAIN] - Epoch=1/4, Step=1150/1785 loss=1.2396 acc=0.5250 lr=0.000020 step/sec=8.31 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:24,788] [   TRAIN] - Epoch=1/4, Step=1160/1785 loss=1.1723 acc=0.5625 lr=0.000020 step/sec=9.16 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:25,953] [   TRAIN] - Epoch=1/4, Step=1170/1785 loss=1.0650 acc=0.6375 lr=0.000020 step/sec=8.58 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:27,078] [   TRAIN] - Epoch=1/4, Step=1180/1785 loss=1.1972 acc=0.5750 lr=0.000020 step/sec=8.89 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:28,163] [   TRAIN] - Epoch=1/4, Step=1190/1785 loss=1.2978 acc=0.5000 lr=0.000020 step/sec=9.22 | ETA 00:13:26\r\n",
      "[2024-08-09 06:33:29,254] [   TRAIN] - Epoch=1/4, Step=1200/1785 loss=1.2358 acc=0.5500 lr=0.000020 step/sec=9.17 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:30,387] [   TRAIN] - Epoch=1/4, Step=1210/1785 loss=1.1423 acc=0.5875 lr=0.000020 step/sec=8.83 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:31,502] [   TRAIN] - Epoch=1/4, Step=1220/1785 loss=1.2156 acc=0.5188 lr=0.000020 step/sec=8.97 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:32,604] [   TRAIN] - Epoch=1/4, Step=1230/1785 loss=1.1578 acc=0.6062 lr=0.000020 step/sec=9.07 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:33,720] [   TRAIN] - Epoch=1/4, Step=1240/1785 loss=1.2846 acc=0.5125 lr=0.000020 step/sec=8.96 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:34,812] [   TRAIN] - Epoch=1/4, Step=1250/1785 loss=1.1985 acc=0.6000 lr=0.000020 step/sec=9.16 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:35,980] [   TRAIN] - Epoch=1/4, Step=1260/1785 loss=1.1637 acc=0.6062 lr=0.000020 step/sec=8.56 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:37,113] [   TRAIN] - Epoch=1/4, Step=1270/1785 loss=1.1715 acc=0.5625 lr=0.000020 step/sec=8.83 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:38,236] [   TRAIN] - Epoch=1/4, Step=1280/1785 loss=1.1550 acc=0.5813 lr=0.000020 step/sec=8.91 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:39,383] [   TRAIN] - Epoch=1/4, Step=1290/1785 loss=1.0712 acc=0.6062 lr=0.000020 step/sec=8.72 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:40,479] [   TRAIN] - Epoch=1/4, Step=1300/1785 loss=0.9980 acc=0.6500 lr=0.000020 step/sec=9.13 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:41,575] [   TRAIN] - Epoch=1/4, Step=1310/1785 loss=0.9880 acc=0.6188 lr=0.000020 step/sec=9.12 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:42,764] [   TRAIN] - Epoch=1/4, Step=1320/1785 loss=1.2329 acc=0.5250 lr=0.000020 step/sec=8.41 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:43,872] [   TRAIN] - Epoch=1/4, Step=1330/1785 loss=1.1596 acc=0.5687 lr=0.000020 step/sec=9.03 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:44,949] [   TRAIN] - Epoch=1/4, Step=1340/1785 loss=1.1205 acc=0.5875 lr=0.000020 step/sec=9.29 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:46,102] [   TRAIN] - Epoch=1/4, Step=1350/1785 loss=1.1869 acc=0.5875 lr=0.000020 step/sec=8.67 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:47,227] [   TRAIN] - Epoch=1/4, Step=1360/1785 loss=1.2081 acc=0.5813 lr=0.000020 step/sec=8.89 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:48,374] [   TRAIN] - Epoch=1/4, Step=1370/1785 loss=1.1175 acc=0.6562 lr=0.000020 step/sec=8.72 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:49,446] [   TRAIN] - Epoch=1/4, Step=1380/1785 loss=1.0869 acc=0.6188 lr=0.000020 step/sec=9.33 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:50,643] [   TRAIN] - Epoch=1/4, Step=1390/1785 loss=1.1594 acc=0.5625 lr=0.000020 step/sec=8.36 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:51,756] [   TRAIN] - Epoch=1/4, Step=1400/1785 loss=1.0754 acc=0.6312 lr=0.000020 step/sec=8.98 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:52,922] [   TRAIN] - Epoch=1/4, Step=1410/1785 loss=1.2342 acc=0.5625 lr=0.000020 step/sec=8.58 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:54,037] [   TRAIN] - Epoch=1/4, Step=1420/1785 loss=1.1035 acc=0.5750 lr=0.000020 step/sec=8.98 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:55,037] [   TRAIN] - Epoch=1/4, Step=1430/1785 loss=1.1470 acc=0.6375 lr=0.000020 step/sec=9.99 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:56,179] [   TRAIN] - Epoch=1/4, Step=1440/1785 loss=0.9480 acc=0.6687 lr=0.000020 step/sec=8.76 | ETA 00:13:25\r\n",
      "[2024-08-09 06:33:57,201] [   TRAIN] - Epoch=1/4, Step=1450/1785 loss=1.0878 acc=0.6250 lr=0.000020 step/sec=9.79 | ETA 00:13:24\r\n",
      "[2024-08-09 06:33:58,228] [   TRAIN] - Epoch=1/4, Step=1460/1785 loss=1.0473 acc=0.5875 lr=0.000020 step/sec=9.73 | ETA 00:13:24\r\n",
      "[2024-08-09 06:33:59,359] [   TRAIN] - Epoch=1/4, Step=1470/1785 loss=1.0526 acc=0.5938 lr=0.000020 step/sec=8.85 | ETA 00:13:24\r\n",
      "[2024-08-09 06:34:00,367] [   TRAIN] - Epoch=1/4, Step=1480/1785 loss=1.2055 acc=0.5437 lr=0.000020 step/sec=9.92 | ETA 00:13:23\r\n",
      "[2024-08-09 06:34:01,473] [   TRAIN] - Epoch=1/4, Step=1490/1785 loss=0.9881 acc=0.6188 lr=0.000020 step/sec=9.05 | ETA 00:13:23\r\n",
      "[2024-08-09 06:34:02,468] [   TRAIN] - Epoch=1/4, Step=1500/1785 loss=1.1610 acc=0.5312 lr=0.000020 step/sec=10.04 | ETA 00:13:22\r\n",
      "[2024-08-09 06:34:03,569] [   TRAIN] - Epoch=1/4, Step=1510/1785 loss=1.1678 acc=0.6062 lr=0.000020 step/sec=9.09 | ETA 00:13:22\r\n",
      "[2024-08-09 06:34:04,710] [   TRAIN] - Epoch=1/4, Step=1520/1785 loss=1.0340 acc=0.6250 lr=0.000020 step/sec=8.77 | ETA 00:13:22\r\n",
      "[2024-08-09 06:34:05,765] [   TRAIN] - Epoch=1/4, Step=1530/1785 loss=1.0818 acc=0.5750 lr=0.000020 step/sec=9.47 | ETA 00:13:22\r\n",
      "[2024-08-09 06:34:06,856] [   TRAIN] - Epoch=1/4, Step=1540/1785 loss=1.1437 acc=0.5813 lr=0.000020 step/sec=9.17 | ETA 00:13:22\r\n",
      "[2024-08-09 06:34:07,877] [   TRAIN] - Epoch=1/4, Step=1550/1785 loss=1.0660 acc=0.6250 lr=0.000020 step/sec=9.79 | ETA 00:13:21\r\n",
      "[2024-08-09 06:34:08,997] [   TRAIN] - Epoch=1/4, Step=1560/1785 loss=1.2534 acc=0.5625 lr=0.000020 step/sec=8.93 | ETA 00:13:21\r\n",
      "[2024-08-09 06:34:10,101] [   TRAIN] - Epoch=1/4, Step=1570/1785 loss=1.0430 acc=0.6375 lr=0.000020 step/sec=9.05 | ETA 00:13:21\r\n",
      "[2024-08-09 06:34:11,134] [   TRAIN] - Epoch=1/4, Step=1580/1785 loss=1.1105 acc=0.5938 lr=0.000020 step/sec=9.69 | ETA 00:13:21\r\n",
      "[2024-08-09 06:34:12,170] [   TRAIN] - Epoch=1/4, Step=1590/1785 loss=1.0869 acc=0.6125 lr=0.000020 step/sec=9.65 | ETA 00:13:20\r\n",
      "[2024-08-09 06:34:13,280] [   TRAIN] - Epoch=1/4, Step=1600/1785 loss=1.0123 acc=0.6125 lr=0.000020 step/sec=9.01 | ETA 00:13:20\r\n",
      "[2024-08-09 06:34:14,307] [   TRAIN] - Epoch=1/4, Step=1610/1785 loss=1.1757 acc=0.5813 lr=0.000020 step/sec=9.73 | ETA 00:13:20\r\n",
      "[2024-08-09 06:34:15,345] [   TRAIN] - Epoch=1/4, Step=1620/1785 loss=1.2764 acc=0.5312 lr=0.000020 step/sec=9.64 | ETA 00:13:20\r\n",
      "[2024-08-09 06:34:16,450] [   TRAIN] - Epoch=1/4, Step=1630/1785 loss=1.0981 acc=0.5563 lr=0.000020 step/sec=9.05 | ETA 00:13:20\r\n",
      "[2024-08-09 06:34:17,499] [   TRAIN] - Epoch=1/4, Step=1640/1785 loss=1.2098 acc=0.5500 lr=0.000020 step/sec=9.54 | ETA 00:13:19\r\n",
      "[2024-08-09 06:34:18,625] [   TRAIN] - Epoch=1/4, Step=1650/1785 loss=1.1206 acc=0.6000 lr=0.000020 step/sec=8.88 | ETA 00:13:19\r\n",
      "[2024-08-09 06:34:19,632] [   TRAIN] - Epoch=1/4, Step=1660/1785 loss=1.0455 acc=0.6188 lr=0.000020 step/sec=9.93 | ETA 00:13:19\r\n",
      "[2024-08-09 06:34:20,724] [   TRAIN] - Epoch=1/4, Step=1670/1785 loss=1.1660 acc=0.5188 lr=0.000020 step/sec=9.16 | ETA 00:13:19\r\n",
      "[2024-08-09 06:34:21,837] [   TRAIN] - Epoch=1/4, Step=1680/1785 loss=1.1423 acc=0.6000 lr=0.000020 step/sec=8.98 | ETA 00:13:19\r\n",
      "[2024-08-09 06:34:22,948] [   TRAIN] - Epoch=1/4, Step=1690/1785 loss=1.1000 acc=0.6312 lr=0.000020 step/sec=9.01 | ETA 00:13:19\r\n",
      "[2024-08-09 06:34:23,987] [   TRAIN] - Epoch=1/4, Step=1700/1785 loss=1.1314 acc=0.5750 lr=0.000020 step/sec=9.62 | ETA 00:13:18\r\n",
      "[2024-08-09 06:34:25,010] [   TRAIN] - Epoch=1/4, Step=1710/1785 loss=1.1190 acc=0.6062 lr=0.000020 step/sec=9.78 | ETA 00:13:18\r\n",
      "[2024-08-09 06:34:26,119] [   TRAIN] - Epoch=1/4, Step=1720/1785 loss=1.0355 acc=0.6562 lr=0.000020 step/sec=9.02 | ETA 00:13:18\r\n",
      "[2024-08-09 06:34:27,148] [   TRAIN] - Epoch=1/4, Step=1730/1785 loss=1.1879 acc=0.5437 lr=0.000020 step/sec=9.72 | ETA 00:13:17\r\n",
      "[2024-08-09 06:34:28,231] [   TRAIN] - Epoch=1/4, Step=1740/1785 loss=1.0728 acc=0.6250 lr=0.000020 step/sec=9.23 | ETA 00:13:17\r\n",
      "[2024-08-09 06:34:29,346] [   TRAIN] - Epoch=1/4, Step=1750/1785 loss=1.1718 acc=0.6062 lr=0.000020 step/sec=8.97 | ETA 00:13:17\r\n",
      "[2024-08-09 06:34:30,363] [   TRAIN] - Epoch=1/4, Step=1760/1785 loss=1.4544 acc=0.4875 lr=0.000020 step/sec=9.84 | ETA 00:13:17\r\n",
      "[2024-08-09 06:34:31,490] [   TRAIN] - Epoch=1/4, Step=1770/1785 loss=1.0464 acc=0.6312 lr=0.000020 step/sec=8.87 | ETA 00:13:17\r\n",
      "[2024-08-09 06:34:32,522] [   TRAIN] - Epoch=1/4, Step=1780/1785 loss=1.2330 acc=0.5375 lr=0.000020 step/sec=9.69 | ETA 00:13:17\r\n",
      "[2024-08-09 06:34:41,684] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation \r\n",
      "[2024-08-09 06:34:45,351] [    EVAL] - Saving best model to ./ckpt/best_model [best acc=0.6095]\r\n",
      "[2024-08-09 06:34:45,355] [    INFO] - Saving model checkpoint to ./ckpt/epoch_1\r\n",
      "[2024-08-09 06:34:50,287] [   TRAIN] - Epoch=2/4, Step=10/1785 loss=1.0690 acc=0.6312 lr=0.000020 step/sec=0.84 | ETA 00:14:21\r\n",
      "[2024-08-09 06:34:51,402] [   TRAIN] - Epoch=2/4, Step=20/1785 loss=1.1004 acc=0.6062 lr=0.000020 step/sec=8.97 | ETA 00:14:20\r\n",
      "[2024-08-09 06:34:52,565] [   TRAIN] - Epoch=2/4, Step=30/1785 loss=1.1292 acc=0.6125 lr=0.000020 step/sec=8.60 | ETA 00:14:20\r\n",
      "[2024-08-09 06:34:53,668] [   TRAIN] - Epoch=2/4, Step=40/1785 loss=1.1455 acc=0.5938 lr=0.000020 step/sec=9.06 | ETA 00:14:20\r\n",
      "[2024-08-09 06:34:54,788] [   TRAIN] - Epoch=2/4, Step=50/1785 loss=0.9838 acc=0.6562 lr=0.000020 step/sec=8.93 | ETA 00:14:19\r\n",
      "[2024-08-09 06:34:55,868] [   TRAIN] - Epoch=2/4, Step=60/1785 loss=0.9606 acc=0.6937 lr=0.000020 step/sec=9.26 | ETA 00:14:19\r\n",
      "[2024-08-09 06:34:57,023] [   TRAIN] - Epoch=2/4, Step=70/1785 loss=0.8935 acc=0.6687 lr=0.000020 step/sec=8.65 | ETA 00:14:19\r\n",
      "[2024-08-09 06:34:58,123] [   TRAIN] - Epoch=2/4, Step=80/1785 loss=1.0634 acc=0.6562 lr=0.000020 step/sec=9.09 | ETA 00:14:18\r\n",
      "[2024-08-09 06:34:59,256] [   TRAIN] - Epoch=2/4, Step=90/1785 loss=1.1297 acc=0.5500 lr=0.000020 step/sec=8.83 | ETA 00:14:18\r\n",
      "[2024-08-09 06:35:00,325] [   TRAIN] - Epoch=2/4, Step=100/1785 loss=0.9349 acc=0.6438 lr=0.000020 step/sec=9.36 | ETA 00:14:18\r\n",
      "[2024-08-09 06:35:01,426] [   TRAIN] - Epoch=2/4, Step=110/1785 loss=0.9271 acc=0.6438 lr=0.000020 step/sec=9.08 | ETA 00:14:17\r\n",
      "[2024-08-09 06:35:02,527] [   TRAIN] - Epoch=2/4, Step=120/1785 loss=1.1308 acc=0.6687 lr=0.000020 step/sec=9.08 | ETA 00:14:17\r\n",
      "[2024-08-09 06:35:03,667] [   TRAIN] - Epoch=2/4, Step=130/1785 loss=1.0716 acc=0.6125 lr=0.000020 step/sec=8.77 | ETA 00:14:17\r\n",
      "[2024-08-09 06:35:04,695] [   TRAIN] - Epoch=2/4, Step=140/1785 loss=0.9116 acc=0.7063 lr=0.000020 step/sec=9.72 | ETA 00:14:16\r\n",
      "[2024-08-09 06:35:05,829] [   TRAIN] - Epoch=2/4, Step=150/1785 loss=0.9382 acc=0.6687 lr=0.000020 step/sec=8.82 | ETA 00:14:16\r\n",
      "[2024-08-09 06:35:06,940] [   TRAIN] - Epoch=2/4, Step=160/1785 loss=0.9584 acc=0.6687 lr=0.000020 step/sec=9.00 | ETA 00:14:15\r\n",
      "[2024-08-09 06:35:08,155] [   TRAIN] - Epoch=2/4, Step=170/1785 loss=0.9295 acc=0.6625 lr=0.000020 step/sec=8.23 | ETA 00:14:15\r\n",
      "[2024-08-09 06:35:09,282] [   TRAIN] - Epoch=2/4, Step=180/1785 loss=0.7934 acc=0.7375 lr=0.000020 step/sec=8.88 | ETA 00:14:15\r\n",
      "[2024-08-09 06:35:10,384] [   TRAIN] - Epoch=2/4, Step=190/1785 loss=0.8561 acc=0.6813 lr=0.000020 step/sec=9.07 | ETA 00:14:15\r\n",
      "[2024-08-09 06:35:11,459] [   TRAIN] - Epoch=2/4, Step=200/1785 loss=1.0302 acc=0.6500 lr=0.000020 step/sec=9.30 | ETA 00:14:14\r\n",
      "[2024-08-09 06:35:12,583] [   TRAIN] - Epoch=2/4, Step=210/1785 loss=1.0084 acc=0.6438 lr=0.000020 step/sec=8.90 | ETA 00:14:14\r\n",
      "[2024-08-09 06:35:13,794] [   TRAIN] - Epoch=2/4, Step=220/1785 loss=0.9417 acc=0.6438 lr=0.000020 step/sec=8.26 | ETA 00:14:14\r\n",
      "[2024-08-09 06:35:14,875] [   TRAIN] - Epoch=2/4, Step=230/1785 loss=0.9137 acc=0.6937 lr=0.000020 step/sec=9.25 | ETA 00:14:14\r\n",
      "[2024-08-09 06:35:16,005] [   TRAIN] - Epoch=2/4, Step=240/1785 loss=1.0707 acc=0.6562 lr=0.000020 step/sec=8.86 | ETA 00:14:13\r\n",
      "[2024-08-09 06:35:17,142] [   TRAIN] - Epoch=2/4, Step=250/1785 loss=1.1342 acc=0.5875 lr=0.000020 step/sec=8.79 | ETA 00:14:13\r\n",
      "[2024-08-09 06:35:18,265] [   TRAIN] - Epoch=2/4, Step=260/1785 loss=0.8754 acc=0.6875 lr=0.000020 step/sec=8.91 | ETA 00:14:13\r\n",
      "[2024-08-09 06:35:19,406] [   TRAIN] - Epoch=2/4, Step=270/1785 loss=0.9066 acc=0.6813 lr=0.000020 step/sec=8.77 | ETA 00:14:13\r\n",
      "[2024-08-09 06:35:20,556] [   TRAIN] - Epoch=2/4, Step=280/1785 loss=1.0092 acc=0.5813 lr=0.000020 step/sec=8.69 | ETA 00:14:13\r\n",
      "[2024-08-09 06:35:21,629] [   TRAIN] - Epoch=2/4, Step=290/1785 loss=1.0625 acc=0.6312 lr=0.000020 step/sec=9.32 | ETA 00:14:12\r\n",
      "[2024-08-09 06:35:22,714] [   TRAIN] - Epoch=2/4, Step=300/1785 loss=1.0180 acc=0.6188 lr=0.000020 step/sec=9.22 | ETA 00:14:12\r\n",
      "[2024-08-09 06:35:23,789] [   TRAIN] - Epoch=2/4, Step=310/1785 loss=1.0108 acc=0.6312 lr=0.000020 step/sec=9.30 | ETA 00:14:11\r\n",
      "[2024-08-09 06:35:24,884] [   TRAIN] - Epoch=2/4, Step=320/1785 loss=1.1755 acc=0.5750 lr=0.000020 step/sec=9.13 | ETA 00:14:11\r\n",
      "[2024-08-09 06:35:25,988] [   TRAIN] - Epoch=2/4, Step=330/1785 loss=1.0473 acc=0.6000 lr=0.000020 step/sec=9.06 | ETA 00:14:11\r\n",
      "[2024-08-09 06:35:27,188] [   TRAIN] - Epoch=2/4, Step=340/1785 loss=1.1510 acc=0.5813 lr=0.000020 step/sec=8.33 | ETA 00:14:11\r\n",
      "[2024-08-09 06:35:28,296] [   TRAIN] - Epoch=2/4, Step=350/1785 loss=1.0040 acc=0.6500 lr=0.000020 step/sec=9.03 | ETA 00:14:11\r\n",
      "[2024-08-09 06:35:29,378] [   TRAIN] - Epoch=2/4, Step=360/1785 loss=1.0419 acc=0.6000 lr=0.000020 step/sec=9.24 | ETA 00:14:10\r\n",
      "[2024-08-09 06:35:30,485] [   TRAIN] - Epoch=2/4, Step=370/1785 loss=1.0783 acc=0.5813 lr=0.000020 step/sec=9.04 | ETA 00:14:10\r\n",
      "[2024-08-09 06:35:31,595] [   TRAIN] - Epoch=2/4, Step=380/1785 loss=0.8595 acc=0.6813 lr=0.000020 step/sec=9.00 | ETA 00:14:10\r\n",
      "[2024-08-09 06:35:32,662] [   TRAIN] - Epoch=2/4, Step=390/1785 loss=1.0642 acc=0.6438 lr=0.000020 step/sec=9.38 | ETA 00:14:09\r\n",
      "[2024-08-09 06:35:33,835] [   TRAIN] - Epoch=2/4, Step=400/1785 loss=1.0072 acc=0.6438 lr=0.000020 step/sec=8.53 | ETA 00:14:09\r\n",
      "[2024-08-09 06:35:34,944] [   TRAIN] - Epoch=2/4, Step=410/1785 loss=1.0582 acc=0.6312 lr=0.000020 step/sec=9.02 | ETA 00:14:09\r\n",
      "[2024-08-09 06:35:36,053] [   TRAIN] - Epoch=2/4, Step=420/1785 loss=0.9840 acc=0.6312 lr=0.000020 step/sec=9.01 | ETA 00:14:09\r\n",
      "[2024-08-09 06:35:37,141] [   TRAIN] - Epoch=2/4, Step=430/1785 loss=0.9646 acc=0.6500 lr=0.000020 step/sec=9.19 | ETA 00:14:08\r\n",
      "[2024-08-09 06:35:38,336] [   TRAIN] - Epoch=2/4, Step=440/1785 loss=1.0788 acc=0.5500 lr=0.000020 step/sec=8.37 | ETA 00:14:08\r\n",
      "[2024-08-09 06:35:39,429] [   TRAIN] - Epoch=2/4, Step=450/1785 loss=1.0555 acc=0.6312 lr=0.000020 step/sec=9.14 | ETA 00:14:08\r\n",
      "[2024-08-09 06:35:40,516] [   TRAIN] - Epoch=2/4, Step=460/1785 loss=0.8908 acc=0.6750 lr=0.000020 step/sec=9.20 | ETA 00:14:08\r\n",
      "[2024-08-09 06:35:41,620] [   TRAIN] - Epoch=2/4, Step=470/1785 loss=1.0860 acc=0.6062 lr=0.000020 step/sec=9.06 | ETA 00:14:07\r\n",
      "[2024-08-09 06:35:42,671] [   TRAIN] - Epoch=2/4, Step=480/1785 loss=0.9473 acc=0.6813 lr=0.000020 step/sec=9.51 | ETA 00:14:07\r\n",
      "[2024-08-09 06:35:43,765] [   TRAIN] - Epoch=2/4, Step=490/1785 loss=1.1463 acc=0.5687 lr=0.000020 step/sec=9.14 | ETA 00:14:07\r\n",
      "[2024-08-09 06:35:44,904] [   TRAIN] - Epoch=2/4, Step=500/1785 loss=1.0088 acc=0.6125 lr=0.000020 step/sec=8.77 | ETA 00:14:07\r\n",
      "[2024-08-09 06:35:46,038] [   TRAIN] - Epoch=2/4, Step=510/1785 loss=0.9263 acc=0.6625 lr=0.000020 step/sec=8.82 | ETA 00:14:06\r\n",
      "[2024-08-09 06:35:47,137] [   TRAIN] - Epoch=2/4, Step=520/1785 loss=1.0115 acc=0.6125 lr=0.000020 step/sec=9.10 | ETA 00:14:06\r\n",
      "[2024-08-09 06:35:48,314] [   TRAIN] - Epoch=2/4, Step=530/1785 loss=1.1183 acc=0.6062 lr=0.000020 step/sec=8.50 | ETA 00:14:06\r\n",
      "[2024-08-09 06:35:49,406] [   TRAIN] - Epoch=2/4, Step=540/1785 loss=0.9504 acc=0.6813 lr=0.000020 step/sec=9.16 | ETA 00:14:06\r\n",
      "[2024-08-09 06:35:50,491] [   TRAIN] - Epoch=2/4, Step=550/1785 loss=0.9842 acc=0.6625 lr=0.000020 step/sec=9.22 | ETA 00:14:06\r\n",
      "[2024-08-09 06:35:51,585] [   TRAIN] - Epoch=2/4, Step=560/1785 loss=0.9410 acc=0.6937 lr=0.000020 step/sec=9.14 | ETA 00:14:05\r\n",
      "[2024-08-09 06:35:52,684] [   TRAIN] - Epoch=2/4, Step=570/1785 loss=0.9429 acc=0.7125 lr=0.000020 step/sec=9.10 | ETA 00:14:05\r\n",
      "[2024-08-09 06:35:53,869] [   TRAIN] - Epoch=2/4, Step=580/1785 loss=0.9766 acc=0.6562 lr=0.000020 step/sec=8.44 | ETA 00:14:05\r\n",
      "[2024-08-09 06:35:54,984] [   TRAIN] - Epoch=2/4, Step=590/1785 loss=1.0865 acc=0.5563 lr=0.000020 step/sec=8.97 | ETA 00:14:05\r\n",
      "[2024-08-09 06:35:56,070] [   TRAIN] - Epoch=2/4, Step=600/1785 loss=1.0217 acc=0.6000 lr=0.000020 step/sec=9.20 | ETA 00:14:05\r\n",
      "[2024-08-09 06:35:57,233] [   TRAIN] - Epoch=2/4, Step=610/1785 loss=1.1139 acc=0.5750 lr=0.000020 step/sec=8.60 | ETA 00:14:04\r\n",
      "[2024-08-09 06:35:58,350] [   TRAIN] - Epoch=2/4, Step=620/1785 loss=1.0545 acc=0.6062 lr=0.000020 step/sec=8.95 | ETA 00:14:04\r\n",
      "[2024-08-09 06:35:59,475] [   TRAIN] - Epoch=2/4, Step=630/1785 loss=0.9517 acc=0.7063 lr=0.000020 step/sec=8.89 | ETA 00:14:04\r\n",
      "[2024-08-09 06:36:00,624] [   TRAIN] - Epoch=2/4, Step=640/1785 loss=1.0736 acc=0.6000 lr=0.000020 step/sec=8.70 | ETA 00:14:04\r\n",
      "[2024-08-09 06:36:01,730] [   TRAIN] - Epoch=2/4, Step=650/1785 loss=0.9921 acc=0.6250 lr=0.000020 step/sec=9.04 | ETA 00:14:04\r\n",
      "[2024-08-09 06:36:02,917] [   TRAIN] - Epoch=2/4, Step=660/1785 loss=1.0456 acc=0.6188 lr=0.000020 step/sec=8.42 | ETA 00:14:04\r\n",
      "[2024-08-09 06:36:04,051] [   TRAIN] - Epoch=2/4, Step=670/1785 loss=0.9838 acc=0.6562 lr=0.000020 step/sec=8.82 | ETA 00:14:04\r\n",
      "[2024-08-09 06:36:05,147] [   TRAIN] - Epoch=2/4, Step=680/1785 loss=1.0852 acc=0.5875 lr=0.000020 step/sec=9.12 | ETA 00:14:03\r\n",
      "[2024-08-09 06:36:06,268] [   TRAIN] - Epoch=2/4, Step=690/1785 loss=1.0368 acc=0.6375 lr=0.000020 step/sec=8.93 | ETA 00:14:03\r\n",
      "[2024-08-09 06:36:07,369] [   TRAIN] - Epoch=2/4, Step=700/1785 loss=1.0720 acc=0.6188 lr=0.000020 step/sec=9.08 | ETA 00:14:03\r\n",
      "[2024-08-09 06:36:08,467] [   TRAIN] - Epoch=2/4, Step=710/1785 loss=0.8831 acc=0.6750 lr=0.000020 step/sec=9.11 | ETA 00:14:03\r\n",
      "[2024-08-09 06:36:09,658] [   TRAIN] - Epoch=2/4, Step=720/1785 loss=0.9975 acc=0.6562 lr=0.000020 step/sec=8.39 | ETA 00:14:03\r\n",
      "[2024-08-09 06:36:10,788] [   TRAIN] - Epoch=2/4, Step=730/1785 loss=0.8878 acc=0.7000 lr=0.000020 step/sec=8.85 | ETA 00:14:03\r\n",
      "[2024-08-09 06:36:11,860] [   TRAIN] - Epoch=2/4, Step=740/1785 loss=1.0306 acc=0.6000 lr=0.000020 step/sec=9.33 | ETA 00:14:02\r\n",
      "[2024-08-09 06:36:12,994] [   TRAIN] - Epoch=2/4, Step=750/1785 loss=0.9213 acc=0.7063 lr=0.000020 step/sec=8.82 | ETA 00:14:02\r\n",
      "[2024-08-09 06:36:14,059] [   TRAIN] - Epoch=2/4, Step=760/1785 loss=1.0816 acc=0.6125 lr=0.000020 step/sec=9.39 | ETA 00:14:02\r\n",
      "[2024-08-09 06:36:15,201] [   TRAIN] - Epoch=2/4, Step=770/1785 loss=0.9899 acc=0.6000 lr=0.000020 step/sec=8.76 | ETA 00:14:02\r\n",
      "[2024-08-09 06:36:16,364] [   TRAIN] - Epoch=2/4, Step=780/1785 loss=0.9222 acc=0.6625 lr=0.000020 step/sec=8.60 | ETA 00:14:02\r\n",
      "[2024-08-09 06:36:17,446] [   TRAIN] - Epoch=2/4, Step=790/1785 loss=1.0967 acc=0.6500 lr=0.000020 step/sec=9.25 | ETA 00:14:01\r\n",
      "[2024-08-09 06:36:18,525] [   TRAIN] - Epoch=2/4, Step=800/1785 loss=1.1298 acc=0.6062 lr=0.000020 step/sec=9.26 | ETA 00:14:01\r\n",
      "[2024-08-09 06:36:19,647] [   TRAIN] - Epoch=2/4, Step=810/1785 loss=0.9713 acc=0.6188 lr=0.000020 step/sec=8.92 | ETA 00:14:01\r\n",
      "[2024-08-09 06:36:20,749] [   TRAIN] - Epoch=2/4, Step=820/1785 loss=1.0776 acc=0.5875 lr=0.000020 step/sec=9.07 | ETA 00:14:01\r\n",
      "[2024-08-09 06:36:21,861] [   TRAIN] - Epoch=2/4, Step=830/1785 loss=0.9790 acc=0.6562 lr=0.000020 step/sec=8.99 | ETA 00:14:01\r\n",
      "[2024-08-09 06:36:22,940] [   TRAIN] - Epoch=2/4, Step=840/1785 loss=0.9868 acc=0.6562 lr=0.000020 step/sec=9.27 | ETA 00:14:00\r\n",
      "[2024-08-09 06:36:24,155] [   TRAIN] - Epoch=2/4, Step=850/1785 loss=0.8969 acc=0.6625 lr=0.000020 step/sec=8.23 | ETA 00:14:00\r\n",
      "[2024-08-09 06:36:25,241] [   TRAIN] - Epoch=2/4, Step=860/1785 loss=0.9799 acc=0.6438 lr=0.000020 step/sec=9.21 | ETA 00:14:00\r\n",
      "[2024-08-09 06:36:26,428] [   TRAIN] - Epoch=2/4, Step=870/1785 loss=0.9830 acc=0.6813 lr=0.000020 step/sec=8.42 | ETA 00:14:00\r\n",
      "[2024-08-09 06:36:27,519] [   TRAIN] - Epoch=2/4, Step=880/1785 loss=0.9843 acc=0.6562 lr=0.000020 step/sec=9.17 | ETA 00:14:00\r\n",
      "[2024-08-09 06:36:28,606] [   TRAIN] - Epoch=2/4, Step=890/1785 loss=1.1453 acc=0.5750 lr=0.000020 step/sec=9.20 | ETA 00:14:00\r\n",
      "[2024-08-09 06:36:29,670] [   TRAIN] - Epoch=2/4, Step=900/1785 loss=0.7628 acc=0.7188 lr=0.000020 step/sec=9.39 | ETA 00:13:59\r\n",
      "[2024-08-09 06:36:30,869] [   TRAIN] - Epoch=2/4, Step=910/1785 loss=1.0435 acc=0.6125 lr=0.000020 step/sec=8.34 | ETA 00:14:00\r\n",
      "[2024-08-09 06:36:31,972] [   TRAIN] - Epoch=2/4, Step=920/1785 loss=1.0943 acc=0.5938 lr=0.000020 step/sec=9.07 | ETA 00:13:59\r\n",
      "[2024-08-09 06:36:33,091] [   TRAIN] - Epoch=2/4, Step=930/1785 loss=0.9239 acc=0.6687 lr=0.000020 step/sec=8.94 | ETA 00:13:59\r\n",
      "[2024-08-09 06:36:34,171] [   TRAIN] - Epoch=2/4, Step=940/1785 loss=1.0206 acc=0.6500 lr=0.000020 step/sec=9.25 | ETA 00:13:59\r\n",
      "[2024-08-09 06:36:35,321] [   TRAIN] - Epoch=2/4, Step=950/1785 loss=0.9975 acc=0.6438 lr=0.000020 step/sec=8.70 | ETA 00:13:59\r\n",
      "[2024-08-09 06:36:36,384] [   TRAIN] - Epoch=2/4, Step=960/1785 loss=0.8390 acc=0.6500 lr=0.000020 step/sec=9.40 | ETA 00:13:59\r\n",
      "[2024-08-09 06:36:37,493] [   TRAIN] - Epoch=2/4, Step=970/1785 loss=0.9477 acc=0.6312 lr=0.000020 step/sec=9.01 | ETA 00:13:58\r\n",
      "[2024-08-09 06:36:38,662] [   TRAIN] - Epoch=2/4, Step=980/1785 loss=1.0620 acc=0.5813 lr=0.000020 step/sec=8.56 | ETA 00:13:58\r\n",
      "[2024-08-09 06:36:39,741] [   TRAIN] - Epoch=2/4, Step=990/1785 loss=1.0378 acc=0.6750 lr=0.000020 step/sec=9.27 | ETA 00:13:58\r\n",
      "[2024-08-09 06:36:40,839] [   TRAIN] - Epoch=2/4, Step=1000/1785 loss=1.0219 acc=0.6625 lr=0.000020 step/sec=9.10 | ETA 00:13:58\r\n",
      "[2024-08-09 06:36:42,009] [   TRAIN] - Epoch=2/4, Step=1010/1785 loss=1.0378 acc=0.6312 lr=0.000020 step/sec=8.55 | ETA 00:13:58\r\n",
      "[2024-08-09 06:36:43,121] [   TRAIN] - Epoch=2/4, Step=1020/1785 loss=1.0391 acc=0.5938 lr=0.000020 step/sec=8.99 | ETA 00:13:58\r\n",
      "[2024-08-09 06:36:44,224] [   TRAIN] - Epoch=2/4, Step=1030/1785 loss=0.9487 acc=0.7000 lr=0.000020 step/sec=9.07 | ETA 00:13:58\r\n",
      "[2024-08-09 06:36:45,334] [   TRAIN] - Epoch=2/4, Step=1040/1785 loss=1.0620 acc=0.6188 lr=0.000020 step/sec=9.01 | ETA 00:13:57\r\n",
      "[2024-08-09 06:36:46,419] [   TRAIN] - Epoch=2/4, Step=1050/1785 loss=1.0101 acc=0.6188 lr=0.000020 step/sec=9.22 | ETA 00:13:57\r\n",
      "[2024-08-09 06:36:47,591] [   TRAIN] - Epoch=2/4, Step=1060/1785 loss=0.9335 acc=0.6188 lr=0.000020 step/sec=8.53 | ETA 00:13:57\r\n",
      "[2024-08-09 06:36:48,674] [   TRAIN] - Epoch=2/4, Step=1070/1785 loss=0.9303 acc=0.6750 lr=0.000020 step/sec=9.23 | ETA 00:13:57\r\n",
      "[2024-08-09 06:36:49,852] [   TRAIN] - Epoch=2/4, Step=1080/1785 loss=1.0143 acc=0.5938 lr=0.000020 step/sec=8.49 | ETA 00:13:57\r\n",
      "[2024-08-09 06:36:50,948] [   TRAIN] - Epoch=2/4, Step=1090/1785 loss=1.0766 acc=0.6188 lr=0.000020 step/sec=9.13 | ETA 00:13:57\r\n",
      "[2024-08-09 06:36:52,048] [   TRAIN] - Epoch=2/4, Step=1100/1785 loss=1.0188 acc=0.6625 lr=0.000020 step/sec=9.09 | ETA 00:13:57\r\n",
      "[2024-08-09 06:36:53,214] [   TRAIN] - Epoch=2/4, Step=1110/1785 loss=0.9325 acc=0.6687 lr=0.000020 step/sec=8.57 | ETA 00:13:57\r\n",
      "[2024-08-09 06:36:54,317] [   TRAIN] - Epoch=2/4, Step=1120/1785 loss=1.0014 acc=0.6500 lr=0.000020 step/sec=9.06 | ETA 00:13:56\r\n",
      "[2024-08-09 06:36:55,380] [   TRAIN] - Epoch=2/4, Step=1130/1785 loss=1.0355 acc=0.6375 lr=0.000020 step/sec=9.41 | ETA 00:13:56\r\n",
      "[2024-08-09 06:36:56,468] [   TRAIN] - Epoch=2/4, Step=1140/1785 loss=1.0625 acc=0.6375 lr=0.000020 step/sec=9.19 | ETA 00:13:56\r\n",
      "[2024-08-09 06:36:57,632] [   TRAIN] - Epoch=2/4, Step=1150/1785 loss=0.9269 acc=0.6813 lr=0.000020 step/sec=8.59 | ETA 00:13:56\r\n",
      "[2024-08-09 06:36:58,717] [   TRAIN] - Epoch=2/4, Step=1160/1785 loss=0.9604 acc=0.6687 lr=0.000020 step/sec=9.22 | ETA 00:13:56\r\n",
      "[2024-08-09 06:36:59,900] [   TRAIN] - Epoch=2/4, Step=1170/1785 loss=0.9910 acc=0.6500 lr=0.000020 step/sec=8.45 | ETA 00:13:56\r\n",
      "[2024-08-09 06:37:01,005] [   TRAIN] - Epoch=2/4, Step=1180/1785 loss=1.0808 acc=0.5813 lr=0.000020 step/sec=9.05 | ETA 00:13:56\r\n",
      "[2024-08-09 06:37:02,124] [   TRAIN] - Epoch=2/4, Step=1190/1785 loss=0.9730 acc=0.6438 lr=0.000020 step/sec=8.93 | ETA 00:13:55\r\n",
      "[2024-08-09 06:37:03,182] [   TRAIN] - Epoch=2/4, Step=1200/1785 loss=1.0408 acc=0.6250 lr=0.000020 step/sec=9.46 | ETA 00:13:55\r\n",
      "[2024-08-09 06:37:04,396] [   TRAIN] - Epoch=2/4, Step=1210/1785 loss=0.8129 acc=0.7375 lr=0.000020 step/sec=8.23 | ETA 00:13:55\r\n",
      "[2024-08-09 06:37:05,512] [   TRAIN] - Epoch=2/4, Step=1220/1785 loss=0.8927 acc=0.6687 lr=0.000020 step/sec=8.97 | ETA 00:13:55\r\n",
      "[2024-08-09 06:37:06,614] [   TRAIN] - Epoch=2/4, Step=1230/1785 loss=1.0848 acc=0.6312 lr=0.000020 step/sec=9.07 | ETA 00:13:55\r\n",
      "[2024-08-09 06:37:07,745] [   TRAIN] - Epoch=2/4, Step=1240/1785 loss=1.0199 acc=0.6125 lr=0.000020 step/sec=8.85 | ETA 00:13:55\r\n",
      "[2024-08-09 06:37:08,753] [   TRAIN] - Epoch=2/4, Step=1250/1785 loss=1.0188 acc=0.6312 lr=0.000020 step/sec=9.91 | ETA 00:13:55\r\n",
      "[2024-08-09 06:37:09,930] [   TRAIN] - Epoch=2/4, Step=1260/1785 loss=1.0170 acc=0.6375 lr=0.000020 step/sec=8.50 | ETA 00:13:55\r\n",
      "[2024-08-09 06:37:10,963] [   TRAIN] - Epoch=2/4, Step=1270/1785 loss=0.9288 acc=0.6750 lr=0.000020 step/sec=9.68 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:12,087] [   TRAIN] - Epoch=2/4, Step=1280/1785 loss=0.9077 acc=0.6875 lr=0.000020 step/sec=8.90 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:13,286] [   TRAIN] - Epoch=2/4, Step=1290/1785 loss=1.2307 acc=0.5188 lr=0.000020 step/sec=8.34 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:14,383] [   TRAIN] - Epoch=2/4, Step=1300/1785 loss=0.9255 acc=0.6562 lr=0.000020 step/sec=9.12 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:15,531] [   TRAIN] - Epoch=2/4, Step=1310/1785 loss=0.9670 acc=0.5875 lr=0.000020 step/sec=8.71 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:16,730] [   TRAIN] - Epoch=2/4, Step=1320/1785 loss=0.9893 acc=0.6312 lr=0.000020 step/sec=8.34 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:17,839] [   TRAIN] - Epoch=2/4, Step=1330/1785 loss=0.8921 acc=0.6687 lr=0.000020 step/sec=9.02 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:18,963] [   TRAIN] - Epoch=2/4, Step=1340/1785 loss=1.0017 acc=0.6813 lr=0.000020 step/sec=8.90 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:20,054] [   TRAIN] - Epoch=2/4, Step=1350/1785 loss=1.0280 acc=0.6813 lr=0.000020 step/sec=9.16 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:21,218] [   TRAIN] - Epoch=2/4, Step=1360/1785 loss=0.9862 acc=0.6375 lr=0.000020 step/sec=8.59 | ETA 00:13:54\r\n",
      "[2024-08-09 06:37:22,324] [   TRAIN] - Epoch=2/4, Step=1370/1785 loss=0.9096 acc=0.6813 lr=0.000020 step/sec=9.04 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:23,434] [   TRAIN] - Epoch=2/4, Step=1380/1785 loss=0.8894 acc=0.6750 lr=0.000020 step/sec=9.00 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:24,532] [   TRAIN] - Epoch=2/4, Step=1390/1785 loss=1.0926 acc=0.6062 lr=0.000020 step/sec=9.11 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:25,757] [   TRAIN] - Epoch=2/4, Step=1400/1785 loss=0.9235 acc=0.6875 lr=0.000020 step/sec=8.17 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:26,857] [   TRAIN] - Epoch=2/4, Step=1410/1785 loss=0.9864 acc=0.6500 lr=0.000020 step/sec=9.09 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:27,976] [   TRAIN] - Epoch=2/4, Step=1420/1785 loss=1.0078 acc=0.6312 lr=0.000020 step/sec=8.94 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:29,062] [   TRAIN] - Epoch=2/4, Step=1430/1785 loss=1.1447 acc=0.6000 lr=0.000020 step/sec=9.21 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:30,137] [   TRAIN] - Epoch=2/4, Step=1440/1785 loss=0.9901 acc=0.6188 lr=0.000020 step/sec=9.30 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:31,224] [   TRAIN] - Epoch=2/4, Step=1450/1785 loss=1.0488 acc=0.6750 lr=0.000020 step/sec=9.20 | ETA 00:13:53\r\n",
      "[2024-08-09 06:37:32,348] [   TRAIN] - Epoch=2/4, Step=1460/1785 loss=0.9346 acc=0.6687 lr=0.000020 step/sec=8.90 | ETA 00:13:52\r\n",
      "[2024-08-09 06:37:33,508] [   TRAIN] - Epoch=2/4, Step=1470/1785 loss=0.8122 acc=0.6875 lr=0.000020 step/sec=8.62 | ETA 00:13:52\r\n",
      "[2024-08-09 06:37:34,592] [   TRAIN] - Epoch=2/4, Step=1480/1785 loss=0.8592 acc=0.7000 lr=0.000020 step/sec=9.23 | ETA 00:13:52\r\n",
      "[2024-08-09 06:37:35,690] [   TRAIN] - Epoch=2/4, Step=1490/1785 loss=0.9969 acc=0.6500 lr=0.000020 step/sec=9.10 | ETA 00:13:52\r\n",
      "[2024-08-09 06:37:36,768] [   TRAIN] - Epoch=2/4, Step=1500/1785 loss=0.8486 acc=0.7000 lr=0.000020 step/sec=9.28 | ETA 00:13:52\r\n",
      "[2024-08-09 06:37:37,959] [   TRAIN] - Epoch=2/4, Step=1510/1785 loss=1.0802 acc=0.6125 lr=0.000020 step/sec=8.39 | ETA 00:13:52\r\n",
      "[2024-08-09 06:37:39,023] [   TRAIN] - Epoch=2/4, Step=1520/1785 loss=1.0245 acc=0.5875 lr=0.000020 step/sec=9.40 | ETA 00:13:52\r\n",
      "[2024-08-09 06:37:40,166] [   TRAIN] - Epoch=2/4, Step=1530/1785 loss=1.0387 acc=0.5938 lr=0.000020 step/sec=8.75 | ETA 00:13:52\r\n",
      "[2024-08-09 06:37:41,255] [   TRAIN] - Epoch=2/4, Step=1540/1785 loss=0.8184 acc=0.7063 lr=0.000020 step/sec=9.19 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:42,365] [   TRAIN] - Epoch=2/4, Step=1550/1785 loss=1.0231 acc=0.6625 lr=0.000020 step/sec=9.00 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:43,471] [   TRAIN] - Epoch=2/4, Step=1560/1785 loss=0.9460 acc=0.6375 lr=0.000020 step/sec=9.05 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:44,637] [   TRAIN] - Epoch=2/4, Step=1570/1785 loss=0.9710 acc=0.6375 lr=0.000020 step/sec=8.57 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:45,710] [   TRAIN] - Epoch=2/4, Step=1580/1785 loss=0.8213 acc=0.6813 lr=0.000020 step/sec=9.32 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:46,800] [   TRAIN] - Epoch=2/4, Step=1590/1785 loss=1.1395 acc=0.5938 lr=0.000020 step/sec=9.17 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:47,979] [   TRAIN] - Epoch=2/4, Step=1600/1785 loss=0.9552 acc=0.6188 lr=0.000020 step/sec=8.48 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:49,066] [   TRAIN] - Epoch=2/4, Step=1610/1785 loss=0.9987 acc=0.6375 lr=0.000020 step/sec=9.20 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:50,169] [   TRAIN] - Epoch=2/4, Step=1620/1785 loss=0.9983 acc=0.6750 lr=0.000020 step/sec=9.06 | ETA 00:13:51\r\n",
      "[2024-08-09 06:37:51,243] [   TRAIN] - Epoch=2/4, Step=1630/1785 loss=1.0007 acc=0.6188 lr=0.000020 step/sec=9.31 | ETA 00:13:50\r\n",
      "[2024-08-09 06:37:52,425] [   TRAIN] - Epoch=2/4, Step=1640/1785 loss=0.9093 acc=0.6813 lr=0.000020 step/sec=8.46 | ETA 00:13:50\r\n",
      "[2024-08-09 06:37:53,441] [   TRAIN] - Epoch=2/4, Step=1650/1785 loss=1.0114 acc=0.6188 lr=0.000020 step/sec=9.84 | ETA 00:13:50\r\n",
      "[2024-08-09 06:37:54,655] [   TRAIN] - Epoch=2/4, Step=1660/1785 loss=1.0244 acc=0.6250 lr=0.000020 step/sec=8.24 | ETA 00:13:50\r\n",
      "[2024-08-09 06:37:55,726] [   TRAIN] - Epoch=2/4, Step=1670/1785 loss=1.0828 acc=0.5625 lr=0.000020 step/sec=9.34 | ETA 00:13:50\r\n",
      "[2024-08-09 06:37:56,807] [   TRAIN] - Epoch=2/4, Step=1680/1785 loss=0.9962 acc=0.6438 lr=0.000020 step/sec=9.25 | ETA 00:13:50\r\n",
      "[2024-08-09 06:37:57,867] [   TRAIN] - Epoch=2/4, Step=1690/1785 loss=0.9634 acc=0.6562 lr=0.000020 step/sec=9.43 | ETA 00:13:50\r\n",
      "[2024-08-09 06:37:59,011] [   TRAIN] - Epoch=2/4, Step=1700/1785 loss=1.0225 acc=0.6312 lr=0.000020 step/sec=8.74 | ETA 00:13:50\r\n",
      "[2024-08-09 06:38:00,102] [   TRAIN] - Epoch=2/4, Step=1710/1785 loss=0.9713 acc=0.6875 lr=0.000020 step/sec=9.17 | ETA 00:13:50\r\n",
      "[2024-08-09 06:38:01,295] [   TRAIN] - Epoch=2/4, Step=1720/1785 loss=1.1750 acc=0.5687 lr=0.000020 step/sec=8.38 | ETA 00:13:50\r\n",
      "[2024-08-09 06:38:02,393] [   TRAIN] - Epoch=2/4, Step=1730/1785 loss=0.9130 acc=0.6937 lr=0.000020 step/sec=9.10 | ETA 00:13:49\r\n",
      "[2024-08-09 06:38:03,583] [   TRAIN] - Epoch=2/4, Step=1740/1785 loss=1.0876 acc=0.5687 lr=0.000020 step/sec=8.41 | ETA 00:13:50\r\n",
      "[2024-08-09 06:38:04,745] [   TRAIN] - Epoch=2/4, Step=1750/1785 loss=1.0026 acc=0.6062 lr=0.000020 step/sec=8.60 | ETA 00:13:50\r\n",
      "[2024-08-09 06:38:05,847] [   TRAIN] - Epoch=2/4, Step=1760/1785 loss=1.0761 acc=0.6438 lr=0.000020 step/sec=9.07 | ETA 00:13:49\r\n",
      "[2024-08-09 06:38:06,927] [   TRAIN] - Epoch=2/4, Step=1770/1785 loss=1.0293 acc=0.6438 lr=0.000020 step/sec=9.26 | ETA 00:13:49\r\n",
      "[2024-08-09 06:38:08,093] [   TRAIN] - Epoch=2/4, Step=1780/1785 loss=1.0558 acc=0.6312 lr=0.000020 step/sec=8.58 | ETA 00:13:49\r\n",
      "[2024-08-09 06:38:17,323] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation \r\n",
      "[2024-08-09 06:38:17,326] [    INFO] - Saving model checkpoint to ./ckpt/epoch_2\r\n",
      "[2024-08-09 06:38:22,098] [   TRAIN] - Epoch=3/4, Step=10/1785 loss=1.0227 acc=0.6375 lr=0.000020 step/sec=1.07 | ETA 00:14:14\r\n",
      "[2024-08-09 06:38:23,171] [   TRAIN] - Epoch=3/4, Step=20/1785 loss=0.8314 acc=0.7188 lr=0.000020 step/sec=9.32 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:24,358] [   TRAIN] - Epoch=3/4, Step=30/1785 loss=0.8675 acc=0.7063 lr=0.000020 step/sec=8.42 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:25,494] [   TRAIN] - Epoch=3/4, Step=40/1785 loss=0.8901 acc=0.7125 lr=0.000020 step/sec=8.80 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:26,665] [   TRAIN] - Epoch=3/4, Step=50/1785 loss=0.8828 acc=0.6750 lr=0.000020 step/sec=8.54 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:27,878] [   TRAIN] - Epoch=3/4, Step=60/1785 loss=0.8181 acc=0.6875 lr=0.000020 step/sec=8.24 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:28,984] [   TRAIN] - Epoch=3/4, Step=70/1785 loss=0.8684 acc=0.7063 lr=0.000020 step/sec=9.05 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:30,097] [   TRAIN] - Epoch=3/4, Step=80/1785 loss=0.8786 acc=0.6438 lr=0.000020 step/sec=8.98 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:31,293] [   TRAIN] - Epoch=3/4, Step=90/1785 loss=0.9357 acc=0.6438 lr=0.000020 step/sec=8.36 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:32,402] [   TRAIN] - Epoch=3/4, Step=100/1785 loss=0.8004 acc=0.6937 lr=0.000020 step/sec=9.01 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:33,537] [   TRAIN] - Epoch=3/4, Step=110/1785 loss=0.7605 acc=0.7438 lr=0.000020 step/sec=8.81 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:34,647] [   TRAIN] - Epoch=3/4, Step=120/1785 loss=0.7392 acc=0.7625 lr=0.000020 step/sec=9.01 | ETA 00:14:13\r\n",
      "[2024-08-09 06:38:35,762] [   TRAIN] - Epoch=3/4, Step=130/1785 loss=0.8011 acc=0.7375 lr=0.000020 step/sec=8.97 | ETA 00:14:12\r\n",
      "[2024-08-09 06:38:36,962] [   TRAIN] - Epoch=3/4, Step=140/1785 loss=0.9719 acc=0.6625 lr=0.000020 step/sec=8.34 | ETA 00:14:12\r\n",
      "[2024-08-09 06:38:38,026] [   TRAIN] - Epoch=3/4, Step=150/1785 loss=0.8268 acc=0.7312 lr=0.000020 step/sec=9.40 | ETA 00:14:12\r\n",
      "[2024-08-09 06:38:39,226] [   TRAIN] - Epoch=3/4, Step=160/1785 loss=0.9073 acc=0.6500 lr=0.000020 step/sec=8.33 | ETA 00:14:12\r\n",
      "[2024-08-09 06:38:40,338] [   TRAIN] - Epoch=3/4, Step=170/1785 loss=0.7512 acc=0.7438 lr=0.000020 step/sec=9.00 | ETA 00:14:12\r\n",
      "[2024-08-09 06:38:41,436] [   TRAIN] - Epoch=3/4, Step=180/1785 loss=0.8376 acc=0.7250 lr=0.000020 step/sec=9.10 | ETA 00:14:12\r\n",
      "[2024-08-09 06:38:42,564] [   TRAIN] - Epoch=3/4, Step=190/1785 loss=0.8828 acc=0.6687 lr=0.000020 step/sec=8.87 | ETA 00:14:12\r\n",
      "[2024-08-09 06:38:43,662] [   TRAIN] - Epoch=3/4, Step=200/1785 loss=0.7955 acc=0.7312 lr=0.000020 step/sec=9.10 | ETA 00:14:11\r\n",
      "[2024-08-09 06:38:44,770] [   TRAIN] - Epoch=3/4, Step=210/1785 loss=0.8989 acc=0.7188 lr=0.000020 step/sec=9.02 | ETA 00:14:11\r\n",
      "[2024-08-09 06:38:45,902] [   TRAIN] - Epoch=3/4, Step=220/1785 loss=0.7121 acc=0.7562 lr=0.000020 step/sec=8.84 | ETA 00:14:11\r\n",
      "[2024-08-09 06:38:47,037] [   TRAIN] - Epoch=3/4, Step=230/1785 loss=0.8986 acc=0.6625 lr=0.000020 step/sec=8.81 | ETA 00:14:11\r\n",
      "[2024-08-09 06:38:48,145] [   TRAIN] - Epoch=3/4, Step=240/1785 loss=0.8738 acc=0.7188 lr=0.000020 step/sec=9.02 | ETA 00:14:11\r\n",
      "[2024-08-09 06:38:49,265] [   TRAIN] - Epoch=3/4, Step=250/1785 loss=0.7711 acc=0.7375 lr=0.000020 step/sec=8.93 | ETA 00:14:11\r\n",
      "[2024-08-09 06:38:50,336] [   TRAIN] - Epoch=3/4, Step=260/1785 loss=0.6971 acc=0.7812 lr=0.000020 step/sec=9.34 | ETA 00:14:11\r\n",
      "[2024-08-09 06:38:51,423] [   TRAIN] - Epoch=3/4, Step=270/1785 loss=1.0225 acc=0.6438 lr=0.000020 step/sec=9.20 | ETA 00:14:10\r\n",
      "[2024-08-09 06:38:52,525] [   TRAIN] - Epoch=3/4, Step=280/1785 loss=0.7198 acc=0.7688 lr=0.000020 step/sec=9.07 | ETA 00:14:10\r\n",
      "[2024-08-09 06:38:53,620] [   TRAIN] - Epoch=3/4, Step=290/1785 loss=0.8391 acc=0.7188 lr=0.000020 step/sec=9.13 | ETA 00:14:10\r\n",
      "[2024-08-09 06:38:54,749] [   TRAIN] - Epoch=3/4, Step=300/1785 loss=0.9630 acc=0.6562 lr=0.000020 step/sec=8.86 | ETA 00:14:10\r\n",
      "[2024-08-09 06:38:55,920] [   TRAIN] - Epoch=3/4, Step=310/1785 loss=0.8975 acc=0.6625 lr=0.000020 step/sec=8.54 | ETA 00:14:10\r\n",
      "[2024-08-09 06:38:57,000] [   TRAIN] - Epoch=3/4, Step=320/1785 loss=0.8605 acc=0.7312 lr=0.000020 step/sec=9.26 | ETA 00:14:10\r\n",
      "[2024-08-09 06:38:58,181] [   TRAIN] - Epoch=3/4, Step=330/1785 loss=0.8455 acc=0.6937 lr=0.000020 step/sec=8.47 | ETA 00:14:10\r\n",
      "[2024-08-09 06:38:59,290] [   TRAIN] - Epoch=3/4, Step=340/1785 loss=0.8811 acc=0.6500 lr=0.000020 step/sec=9.02 | ETA 00:14:10\r\n",
      "[2024-08-09 06:39:00,393] [   TRAIN] - Epoch=3/4, Step=350/1785 loss=0.8342 acc=0.7438 lr=0.000020 step/sec=9.07 | ETA 00:14:09\r\n",
      "[2024-08-09 06:39:01,501] [   TRAIN] - Epoch=3/4, Step=360/1785 loss=0.8788 acc=0.7188 lr=0.000020 step/sec=9.02 | ETA 00:14:09\r\n",
      "[2024-08-09 06:39:02,600] [   TRAIN] - Epoch=3/4, Step=370/1785 loss=0.8112 acc=0.6813 lr=0.000020 step/sec=9.10 | ETA 00:14:09\r\n",
      "[2024-08-09 06:39:03,771] [   TRAIN] - Epoch=3/4, Step=380/1785 loss=0.8433 acc=0.6875 lr=0.000020 step/sec=8.54 | ETA 00:14:09\r\n",
      "[2024-08-09 06:39:04,868] [   TRAIN] - Epoch=3/4, Step=390/1785 loss=0.9505 acc=0.6625 lr=0.000020 step/sec=9.11 | ETA 00:14:09\r\n",
      "[2024-08-09 06:39:05,930] [   TRAIN] - Epoch=3/4, Step=400/1785 loss=0.9057 acc=0.6750 lr=0.000020 step/sec=9.42 | ETA 00:14:09\r\n",
      "[2024-08-09 06:39:07,145] [   TRAIN] - Epoch=3/4, Step=410/1785 loss=0.9364 acc=0.6687 lr=0.000020 step/sec=8.23 | ETA 00:14:09\r\n",
      "[2024-08-09 06:39:08,243] [   TRAIN] - Epoch=3/4, Step=420/1785 loss=0.9256 acc=0.7000 lr=0.000020 step/sec=9.11 | ETA 00:14:08\r\n",
      "[2024-08-09 06:39:09,357] [   TRAIN] - Epoch=3/4, Step=430/1785 loss=0.8946 acc=0.7125 lr=0.000020 step/sec=8.98 | ETA 00:14:08\r\n",
      "[2024-08-09 06:39:10,456] [   TRAIN] - Epoch=3/4, Step=440/1785 loss=0.8090 acc=0.7250 lr=0.000020 step/sec=9.09 | ETA 00:14:08\r\n",
      "[2024-08-09 06:39:11,555] [   TRAIN] - Epoch=3/4, Step=450/1785 loss=0.8587 acc=0.7125 lr=0.000020 step/sec=9.10 | ETA 00:14:08\r\n",
      "[2024-08-09 06:39:12,731] [   TRAIN] - Epoch=3/4, Step=460/1785 loss=0.9395 acc=0.6875 lr=0.000020 step/sec=8.50 | ETA 00:14:08\r\n",
      "[2024-08-09 06:39:13,825] [   TRAIN] - Epoch=3/4, Step=470/1785 loss=0.7934 acc=0.7250 lr=0.000020 step/sec=9.14 | ETA 00:14:08\r\n",
      "[2024-08-09 06:39:14,940] [   TRAIN] - Epoch=3/4, Step=480/1785 loss=0.8884 acc=0.6937 lr=0.000020 step/sec=8.97 | ETA 00:14:08\r\n",
      "[2024-08-09 06:39:16,015] [   TRAIN] - Epoch=3/4, Step=490/1785 loss=0.7483 acc=0.7438 lr=0.000020 step/sec=9.30 | ETA 00:14:08\r\n",
      "[2024-08-09 06:39:17,115] [   TRAIN] - Epoch=3/4, Step=500/1785 loss=0.7665 acc=0.7625 lr=0.000020 step/sec=9.08 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:18,288] [   TRAIN] - Epoch=3/4, Step=510/1785 loss=0.8469 acc=0.7188 lr=0.000020 step/sec=8.53 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:19,367] [   TRAIN] - Epoch=3/4, Step=520/1785 loss=0.9718 acc=0.6312 lr=0.000020 step/sec=9.27 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:20,540] [   TRAIN] - Epoch=3/4, Step=530/1785 loss=0.8492 acc=0.7063 lr=0.000020 step/sec=8.52 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:21,619] [   TRAIN] - Epoch=3/4, Step=540/1785 loss=0.9760 acc=0.6625 lr=0.000020 step/sec=9.27 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:22,719] [   TRAIN] - Epoch=3/4, Step=550/1785 loss=0.7819 acc=0.7063 lr=0.000020 step/sec=9.09 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:23,922] [   TRAIN] - Epoch=3/4, Step=560/1785 loss=0.7108 acc=0.7688 lr=0.000020 step/sec=8.32 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:25,011] [   TRAIN] - Epoch=3/4, Step=570/1785 loss=0.7279 acc=0.7438 lr=0.000020 step/sec=9.18 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:26,112] [   TRAIN] - Epoch=3/4, Step=580/1785 loss=0.7408 acc=0.7500 lr=0.000020 step/sec=9.08 | ETA 00:14:07\r\n",
      "[2024-08-09 06:39:27,223] [   TRAIN] - Epoch=3/4, Step=590/1785 loss=0.7368 acc=0.7562 lr=0.000020 step/sec=9.00 | ETA 00:14:06\r\n",
      "[2024-08-09 06:39:28,371] [   TRAIN] - Epoch=3/4, Step=600/1785 loss=0.8611 acc=0.6875 lr=0.000020 step/sec=8.71 | ETA 00:14:06\r\n",
      "[2024-08-09 06:39:29,449] [   TRAIN] - Epoch=3/4, Step=610/1785 loss=0.8214 acc=0.7500 lr=0.000020 step/sec=9.27 | ETA 00:14:06\r\n",
      "[2024-08-09 06:39:30,521] [   TRAIN] - Epoch=3/4, Step=620/1785 loss=0.7878 acc=0.7812 lr=0.000020 step/sec=9.33 | ETA 00:14:06\r\n",
      "[2024-08-09 06:39:31,617] [   TRAIN] - Epoch=3/4, Step=630/1785 loss=0.9275 acc=0.6562 lr=0.000020 step/sec=9.12 | ETA 00:14:06\r\n",
      "[2024-08-09 06:39:32,708] [   TRAIN] - Epoch=3/4, Step=640/1785 loss=0.7626 acc=0.7125 lr=0.000020 step/sec=9.17 | ETA 00:14:06\r\n",
      "[2024-08-09 06:39:33,803] [   TRAIN] - Epoch=3/4, Step=650/1785 loss=0.8450 acc=0.7063 lr=0.000020 step/sec=9.14 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:34,898] [   TRAIN] - Epoch=3/4, Step=660/1785 loss=0.7696 acc=0.7375 lr=0.000020 step/sec=9.13 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:36,069] [   TRAIN] - Epoch=3/4, Step=670/1785 loss=0.9421 acc=0.6625 lr=0.000020 step/sec=8.54 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:37,164] [   TRAIN] - Epoch=3/4, Step=680/1785 loss=0.9656 acc=0.6813 lr=0.000020 step/sec=9.13 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:38,271] [   TRAIN] - Epoch=3/4, Step=690/1785 loss=0.8420 acc=0.6937 lr=0.000020 step/sec=9.04 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:39,377] [   TRAIN] - Epoch=3/4, Step=700/1785 loss=0.7658 acc=0.7312 lr=0.000020 step/sec=9.04 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:40,548] [   TRAIN] - Epoch=3/4, Step=710/1785 loss=0.8309 acc=0.6813 lr=0.000020 step/sec=8.54 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:41,667] [   TRAIN] - Epoch=3/4, Step=720/1785 loss=0.8631 acc=0.6875 lr=0.000020 step/sec=8.93 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:42,866] [   TRAIN] - Epoch=3/4, Step=730/1785 loss=0.8122 acc=0.7063 lr=0.000020 step/sec=8.34 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:43,945] [   TRAIN] - Epoch=3/4, Step=740/1785 loss=0.7281 acc=0.7937 lr=0.000020 step/sec=9.27 | ETA 00:14:05\r\n",
      "[2024-08-09 06:39:45,023] [   TRAIN] - Epoch=3/4, Step=750/1785 loss=0.7731 acc=0.7063 lr=0.000020 step/sec=9.28 | ETA 00:14:04\r\n",
      "[2024-08-09 06:39:46,094] [   TRAIN] - Epoch=3/4, Step=760/1785 loss=1.0315 acc=0.6500 lr=0.000020 step/sec=9.34 | ETA 00:14:04\r\n",
      "[2024-08-09 06:39:47,242] [   TRAIN] - Epoch=3/4, Step=770/1785 loss=0.8948 acc=0.6687 lr=0.000020 step/sec=8.71 | ETA 00:14:04\r\n",
      "[2024-08-09 06:39:48,315] [   TRAIN] - Epoch=3/4, Step=780/1785 loss=0.7700 acc=0.7312 lr=0.000020 step/sec=9.33 | ETA 00:14:04\r\n",
      "[2024-08-09 06:39:49,373] [   TRAIN] - Epoch=3/4, Step=790/1785 loss=0.8140 acc=0.7188 lr=0.000020 step/sec=9.45 | ETA 00:14:04\r\n",
      "[2024-08-09 06:39:50,534] [   TRAIN] - Epoch=3/4, Step=800/1785 loss=0.8816 acc=0.6875 lr=0.000020 step/sec=8.61 | ETA 00:14:04\r\n",
      "[2024-08-09 06:39:51,611] [   TRAIN] - Epoch=3/4, Step=810/1785 loss=0.8171 acc=0.7000 lr=0.000020 step/sec=9.28 | ETA 00:14:04\r\n",
      "[2024-08-09 06:39:52,652] [   TRAIN] - Epoch=3/4, Step=820/1785 loss=0.7511 acc=0.7312 lr=0.000020 step/sec=9.61 | ETA 00:14:03\r\n",
      "[2024-08-09 06:39:53,823] [   TRAIN] - Epoch=3/4, Step=830/1785 loss=0.7978 acc=0.6875 lr=0.000020 step/sec=8.54 | ETA 00:14:03\r\n",
      "[2024-08-09 06:39:54,908] [   TRAIN] - Epoch=3/4, Step=840/1785 loss=0.8733 acc=0.6750 lr=0.000020 step/sec=9.22 | ETA 00:14:03\r\n",
      "[2024-08-09 06:39:56,027] [   TRAIN] - Epoch=3/4, Step=850/1785 loss=0.7390 acc=0.7188 lr=0.000020 step/sec=8.94 | ETA 00:14:03\r\n",
      "[2024-08-09 06:39:57,154] [   TRAIN] - Epoch=3/4, Step=860/1785 loss=0.7585 acc=0.7438 lr=0.000020 step/sec=8.87 | ETA 00:14:03\r\n",
      "[2024-08-09 06:39:58,239] [   TRAIN] - Epoch=3/4, Step=870/1785 loss=0.9224 acc=0.7000 lr=0.000020 step/sec=9.22 | ETA 00:14:03\r\n",
      "[2024-08-09 06:39:59,386] [   TRAIN] - Epoch=3/4, Step=880/1785 loss=0.8905 acc=0.6687 lr=0.000020 step/sec=8.72 | ETA 00:14:03\r\n",
      "[2024-08-09 06:40:00,498] [   TRAIN] - Epoch=3/4, Step=890/1785 loss=0.7796 acc=0.7125 lr=0.000020 step/sec=9.00 | ETA 00:14:03\r\n",
      "[2024-08-09 06:40:01,616] [   TRAIN] - Epoch=3/4, Step=900/1785 loss=0.8472 acc=0.7125 lr=0.000020 step/sec=8.95 | ETA 00:14:03\r\n",
      "[2024-08-09 06:40:02,680] [   TRAIN] - Epoch=3/4, Step=910/1785 loss=0.8315 acc=0.7000 lr=0.000020 step/sec=9.39 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:03,874] [   TRAIN] - Epoch=3/4, Step=920/1785 loss=0.8670 acc=0.6562 lr=0.000020 step/sec=8.38 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:04,953] [   TRAIN] - Epoch=3/4, Step=930/1785 loss=0.7416 acc=0.7375 lr=0.000020 step/sec=9.27 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:06,053] [   TRAIN] - Epoch=3/4, Step=940/1785 loss=0.8860 acc=0.6625 lr=0.000020 step/sec=9.09 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:07,152] [   TRAIN] - Epoch=3/4, Step=950/1785 loss=0.8292 acc=0.6937 lr=0.000020 step/sec=9.10 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:08,245] [   TRAIN] - Epoch=3/4, Step=960/1785 loss=0.7536 acc=0.7250 lr=0.000020 step/sec=9.15 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:09,344] [   TRAIN] - Epoch=3/4, Step=970/1785 loss=0.9098 acc=0.6937 lr=0.000020 step/sec=9.10 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:10,436] [   TRAIN] - Epoch=3/4, Step=980/1785 loss=0.9638 acc=0.6750 lr=0.000020 step/sec=9.16 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:11,614] [   TRAIN] - Epoch=3/4, Step=990/1785 loss=0.9073 acc=0.6562 lr=0.000020 step/sec=8.49 | ETA 00:14:02\r\n",
      "[2024-08-09 06:40:12,707] [   TRAIN] - Epoch=3/4, Step=1000/1785 loss=0.8374 acc=0.7188 lr=0.000020 step/sec=9.15 | ETA 00:14:01\r\n",
      "[2024-08-09 06:40:13,736] [   TRAIN] - Epoch=3/4, Step=1010/1785 loss=0.9728 acc=0.6625 lr=0.000020 step/sec=9.72 | ETA 00:14:01\r\n",
      "[2024-08-09 06:40:14,840] [   TRAIN] - Epoch=3/4, Step=1020/1785 loss=0.8395 acc=0.7000 lr=0.000020 step/sec=9.06 | ETA 00:14:01\r\n",
      "[2024-08-09 06:40:15,928] [   TRAIN] - Epoch=3/4, Step=1030/1785 loss=0.9058 acc=0.7000 lr=0.000020 step/sec=9.19 | ETA 00:14:01\r\n",
      "[2024-08-09 06:40:17,060] [   TRAIN] - Epoch=3/4, Step=1040/1785 loss=0.7421 acc=0.7188 lr=0.000020 step/sec=8.83 | ETA 00:14:01\r\n",
      "[2024-08-09 06:40:18,155] [   TRAIN] - Epoch=3/4, Step=1050/1785 loss=0.8021 acc=0.7375 lr=0.000020 step/sec=9.13 | ETA 00:14:01\r\n",
      "[2024-08-09 06:40:19,252] [   TRAIN] - Epoch=3/4, Step=1060/1785 loss=0.9115 acc=0.6875 lr=0.000020 step/sec=9.12 | ETA 00:14:01\r\n",
      "[2024-08-09 06:40:20,328] [   TRAIN] - Epoch=3/4, Step=1070/1785 loss=0.8639 acc=0.6750 lr=0.000020 step/sec=9.29 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:21,361] [   TRAIN] - Epoch=3/4, Step=1080/1785 loss=0.9094 acc=0.6875 lr=0.000020 step/sec=9.69 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:22,435] [   TRAIN] - Epoch=3/4, Step=1090/1785 loss=0.8803 acc=0.6875 lr=0.000020 step/sec=9.31 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:23,554] [   TRAIN] - Epoch=3/4, Step=1100/1785 loss=0.7410 acc=0.7375 lr=0.000020 step/sec=8.93 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:24,559] [   TRAIN] - Epoch=3/4, Step=1110/1785 loss=0.7539 acc=0.7438 lr=0.000020 step/sec=9.95 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:25,663] [   TRAIN] - Epoch=3/4, Step=1120/1785 loss=0.7763 acc=0.6875 lr=0.000020 step/sec=9.06 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:26,825] [   TRAIN] - Epoch=3/4, Step=1130/1785 loss=0.7994 acc=0.6750 lr=0.000020 step/sec=8.60 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:27,914] [   TRAIN] - Epoch=3/4, Step=1140/1785 loss=0.8805 acc=0.7188 lr=0.000020 step/sec=9.19 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:29,116] [   TRAIN] - Epoch=3/4, Step=1150/1785 loss=0.7928 acc=0.7188 lr=0.000020 step/sec=8.32 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:30,257] [   TRAIN] - Epoch=3/4, Step=1160/1785 loss=0.8059 acc=0.7312 lr=0.000020 step/sec=8.76 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:31,485] [   TRAIN] - Epoch=3/4, Step=1170/1785 loss=0.8899 acc=0.6750 lr=0.000020 step/sec=8.15 | ETA 00:14:00\r\n",
      "[2024-08-09 06:40:32,524] [   TRAIN] - Epoch=3/4, Step=1180/1785 loss=0.8315 acc=0.7312 lr=0.000020 step/sec=9.62 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:33,685] [   TRAIN] - Epoch=3/4, Step=1190/1785 loss=0.9160 acc=0.7125 lr=0.000020 step/sec=8.61 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:34,894] [   TRAIN] - Epoch=3/4, Step=1200/1785 loss=0.7284 acc=0.7500 lr=0.000020 step/sec=8.27 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:36,022] [   TRAIN] - Epoch=3/4, Step=1210/1785 loss=0.8051 acc=0.7250 lr=0.000020 step/sec=8.87 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:37,131] [   TRAIN] - Epoch=3/4, Step=1220/1785 loss=0.7702 acc=0.7375 lr=0.000020 step/sec=9.02 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:38,246] [   TRAIN] - Epoch=3/4, Step=1230/1785 loss=0.8880 acc=0.6562 lr=0.000020 step/sec=8.97 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:39,416] [   TRAIN] - Epoch=3/4, Step=1240/1785 loss=0.8755 acc=0.7125 lr=0.000020 step/sec=8.55 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:40,585] [   TRAIN] - Epoch=3/4, Step=1250/1785 loss=0.7336 acc=0.7625 lr=0.000020 step/sec=8.55 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:41,664] [   TRAIN] - Epoch=3/4, Step=1260/1785 loss=0.8099 acc=0.7250 lr=0.000020 step/sec=9.28 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:42,794] [   TRAIN] - Epoch=3/4, Step=1270/1785 loss=0.6514 acc=0.7937 lr=0.000020 step/sec=8.85 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:43,893] [   TRAIN] - Epoch=3/4, Step=1280/1785 loss=0.9863 acc=0.6750 lr=0.000020 step/sec=9.10 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:45,052] [   TRAIN] - Epoch=3/4, Step=1290/1785 loss=0.8809 acc=0.6813 lr=0.000020 step/sec=8.63 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:46,116] [   TRAIN] - Epoch=3/4, Step=1300/1785 loss=0.9215 acc=0.6750 lr=0.000020 step/sec=9.40 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:47,314] [   TRAIN] - Epoch=3/4, Step=1310/1785 loss=0.7584 acc=0.7125 lr=0.000020 step/sec=8.35 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:48,455] [   TRAIN] - Epoch=3/4, Step=1320/1785 loss=0.9269 acc=0.6687 lr=0.000020 step/sec=8.76 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:49,633] [   TRAIN] - Epoch=3/4, Step=1330/1785 loss=0.9951 acc=0.6438 lr=0.000020 step/sec=8.48 | ETA 00:13:59\r\n",
      "[2024-08-09 06:40:50,738] [   TRAIN] - Epoch=3/4, Step=1340/1785 loss=0.7399 acc=0.7500 lr=0.000020 step/sec=9.05 | ETA 00:13:58\r\n",
      "[2024-08-09 06:40:51,859] [   TRAIN] - Epoch=3/4, Step=1350/1785 loss=0.7238 acc=0.7625 lr=0.000020 step/sec=8.92 | ETA 00:13:58\r\n",
      "[2024-08-09 06:40:52,958] [   TRAIN] - Epoch=3/4, Step=1360/1785 loss=0.7914 acc=0.7312 lr=0.000020 step/sec=9.10 | ETA 00:13:58\r\n",
      "[2024-08-09 06:40:54,102] [   TRAIN] - Epoch=3/4, Step=1370/1785 loss=0.8708 acc=0.6500 lr=0.000020 step/sec=8.74 | ETA 00:13:58\r\n",
      "[2024-08-09 06:40:55,196] [   TRAIN] - Epoch=3/4, Step=1380/1785 loss=0.7653 acc=0.7500 lr=0.000020 step/sec=9.14 | ETA 00:13:58\r\n",
      "[2024-08-09 06:40:56,398] [   TRAIN] - Epoch=3/4, Step=1390/1785 loss=0.7970 acc=0.6750 lr=0.000020 step/sec=8.32 | ETA 00:13:58\r\n",
      "[2024-08-09 06:40:57,477] [   TRAIN] - Epoch=3/4, Step=1400/1785 loss=0.7309 acc=0.7500 lr=0.000020 step/sec=9.27 | ETA 00:13:58\r\n",
      "[2024-08-09 06:40:58,646] [   TRAIN] - Epoch=3/4, Step=1410/1785 loss=0.8576 acc=0.6813 lr=0.000020 step/sec=8.56 | ETA 00:13:58\r\n",
      "[2024-08-09 06:40:59,731] [   TRAIN] - Epoch=3/4, Step=1420/1785 loss=0.8252 acc=0.7625 lr=0.000020 step/sec=9.22 | ETA 00:13:58\r\n",
      "[2024-08-09 06:41:00,914] [   TRAIN] - Epoch=3/4, Step=1430/1785 loss=0.8356 acc=0.7125 lr=0.000020 step/sec=8.46 | ETA 00:13:58\r\n",
      "[2024-08-09 06:41:02,064] [   TRAIN] - Epoch=3/4, Step=1440/1785 loss=0.7880 acc=0.7562 lr=0.000020 step/sec=8.69 | ETA 00:13:58\r\n",
      "[2024-08-09 06:41:03,138] [   TRAIN] - Epoch=3/4, Step=1450/1785 loss=0.8126 acc=0.7000 lr=0.000020 step/sec=9.31 | ETA 00:13:58\r\n",
      "[2024-08-09 06:41:04,234] [   TRAIN] - Epoch=3/4, Step=1460/1785 loss=0.7667 acc=0.7312 lr=0.000020 step/sec=9.12 | ETA 00:13:58\r\n",
      "[2024-08-09 06:41:05,464] [   TRAIN] - Epoch=3/4, Step=1470/1785 loss=0.7400 acc=0.7750 lr=0.000020 step/sec=8.13 | ETA 00:13:58\r\n",
      "[2024-08-09 06:41:06,561] [   TRAIN] - Epoch=3/4, Step=1480/1785 loss=0.8345 acc=0.6625 lr=0.000020 step/sec=9.12 | ETA 00:13:58\r\n",
      "[2024-08-09 06:41:07,644] [   TRAIN] - Epoch=3/4, Step=1490/1785 loss=0.8016 acc=0.7125 lr=0.000020 step/sec=9.23 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:08,730] [   TRAIN] - Epoch=3/4, Step=1500/1785 loss=0.8333 acc=0.7000 lr=0.000020 step/sec=9.20 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:09,926] [   TRAIN] - Epoch=3/4, Step=1510/1785 loss=0.7733 acc=0.7188 lr=0.000020 step/sec=8.37 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:11,094] [   TRAIN] - Epoch=3/4, Step=1520/1785 loss=0.8270 acc=0.7000 lr=0.000020 step/sec=8.56 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:12,201] [   TRAIN] - Epoch=3/4, Step=1530/1785 loss=0.8671 acc=0.7188 lr=0.000020 step/sec=9.03 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:13,312] [   TRAIN] - Epoch=3/4, Step=1540/1785 loss=0.8188 acc=0.7438 lr=0.000020 step/sec=9.00 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:14,422] [   TRAIN] - Epoch=3/4, Step=1550/1785 loss=1.0797 acc=0.6438 lr=0.000020 step/sec=9.01 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:15,528] [   TRAIN] - Epoch=3/4, Step=1560/1785 loss=0.7994 acc=0.7000 lr=0.000020 step/sec=9.05 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:16,723] [   TRAIN] - Epoch=3/4, Step=1570/1785 loss=0.9090 acc=0.6500 lr=0.000020 step/sec=8.36 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:17,790] [   TRAIN] - Epoch=3/4, Step=1580/1785 loss=0.8044 acc=0.7000 lr=0.000020 step/sec=9.37 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:18,907] [   TRAIN] - Epoch=3/4, Step=1590/1785 loss=0.7214 acc=0.7063 lr=0.000020 step/sec=8.95 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:20,100] [   TRAIN] - Epoch=3/4, Step=1600/1785 loss=0.7660 acc=0.7500 lr=0.000020 step/sec=8.38 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:21,211] [   TRAIN] - Epoch=3/4, Step=1610/1785 loss=0.7399 acc=0.7438 lr=0.000020 step/sec=9.00 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:22,307] [   TRAIN] - Epoch=3/4, Step=1620/1785 loss=0.8785 acc=0.6937 lr=0.000020 step/sec=9.13 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:23,410] [   TRAIN] - Epoch=3/4, Step=1630/1785 loss=0.8186 acc=0.6875 lr=0.000020 step/sec=9.07 | ETA 00:13:57\r\n",
      "[2024-08-09 06:41:24,516] [   TRAIN] - Epoch=3/4, Step=1640/1785 loss=0.9013 acc=0.7125 lr=0.000020 step/sec=9.04 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:25,606] [   TRAIN] - Epoch=3/4, Step=1650/1785 loss=0.9048 acc=0.6875 lr=0.000020 step/sec=9.18 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:26,731] [   TRAIN] - Epoch=3/4, Step=1660/1785 loss=0.8161 acc=0.7312 lr=0.000020 step/sec=8.89 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:27,826] [   TRAIN] - Epoch=3/4, Step=1670/1785 loss=0.7253 acc=0.7562 lr=0.000020 step/sec=9.13 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:28,969] [   TRAIN] - Epoch=3/4, Step=1680/1785 loss=0.9064 acc=0.7000 lr=0.000020 step/sec=8.75 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:30,102] [   TRAIN] - Epoch=3/4, Step=1690/1785 loss=0.8487 acc=0.6937 lr=0.000020 step/sec=8.82 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:31,227] [   TRAIN] - Epoch=3/4, Step=1700/1785 loss=0.8854 acc=0.6687 lr=0.000020 step/sec=8.89 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:32,334] [   TRAIN] - Epoch=3/4, Step=1710/1785 loss=0.9382 acc=0.6125 lr=0.000020 step/sec=9.04 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:33,440] [   TRAIN] - Epoch=3/4, Step=1720/1785 loss=0.8887 acc=0.6937 lr=0.000020 step/sec=9.04 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:34,556] [   TRAIN] - Epoch=3/4, Step=1730/1785 loss=0.8025 acc=0.7125 lr=0.000020 step/sec=8.96 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:35,632] [   TRAIN] - Epoch=3/4, Step=1740/1785 loss=0.9363 acc=0.6438 lr=0.000020 step/sec=9.29 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:36,735] [   TRAIN] - Epoch=3/4, Step=1750/1785 loss=0.8167 acc=0.6875 lr=0.000020 step/sec=9.07 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:37,911] [   TRAIN] - Epoch=3/4, Step=1760/1785 loss=0.8203 acc=0.7250 lr=0.000020 step/sec=8.50 | ETA 00:13:56\r\n",
      "[2024-08-09 06:41:38,999] [   TRAIN] - Epoch=3/4, Step=1770/1785 loss=0.7488 acc=0.7625 lr=0.000020 step/sec=9.19 | ETA 00:13:55\r\n",
      "[2024-08-09 06:41:40,077] [   TRAIN] - Epoch=3/4, Step=1780/1785 loss=0.7603 acc=0.7125 lr=0.000020 step/sec=9.28 | ETA 00:13:55\r\n",
      "[2024-08-09 06:41:49,641] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation \r\n",
      "[2024-08-09 06:42:00,138] [    EVAL] - Saving best model to ./ckpt/best_model [best acc=0.6143]\r\n",
      "[2024-08-09 06:42:00,142] [    INFO] - Saving model checkpoint to ./ckpt/epoch_3\r\n",
      "[2024-08-09 06:42:04,899] [   TRAIN] - Epoch=4/4, Step=10/1785 loss=0.6389 acc=0.7750 lr=0.000020 step/sec=0.60 | ETA 00:14:26\r\n",
      "[2024-08-09 06:42:06,083] [   TRAIN] - Epoch=4/4, Step=20/1785 loss=0.6892 acc=0.7500 lr=0.000020 step/sec=8.44 | ETA 00:14:26\r\n",
      "[2024-08-09 06:42:07,176] [   TRAIN] - Epoch=4/4, Step=30/1785 loss=0.6326 acc=0.7688 lr=0.000020 step/sec=9.16 | ETA 00:14:26\r\n",
      "[2024-08-09 06:42:08,265] [   TRAIN] - Epoch=4/4, Step=40/1785 loss=0.6588 acc=0.7500 lr=0.000020 step/sec=9.18 | ETA 00:14:26\r\n",
      "[2024-08-09 06:42:09,363] [   TRAIN] - Epoch=4/4, Step=50/1785 loss=0.7876 acc=0.7188 lr=0.000020 step/sec=9.10 | ETA 00:14:25\r\n",
      "[2024-08-09 06:42:10,586] [   TRAIN] - Epoch=4/4, Step=60/1785 loss=0.6614 acc=0.7688 lr=0.000020 step/sec=8.18 | ETA 00:14:26\r\n",
      "[2024-08-09 06:42:11,686] [   TRAIN] - Epoch=4/4, Step=70/1785 loss=0.6959 acc=0.7937 lr=0.000020 step/sec=9.08 | ETA 00:14:25\r\n",
      "[2024-08-09 06:42:12,784] [   TRAIN] - Epoch=4/4, Step=80/1785 loss=0.5490 acc=0.8187 lr=0.000020 step/sec=9.11 | ETA 00:14:25\r\n",
      "[2024-08-09 06:42:13,909] [   TRAIN] - Epoch=4/4, Step=90/1785 loss=0.6840 acc=0.7500 lr=0.000020 step/sec=8.89 | ETA 00:14:25\r\n",
      "[2024-08-09 06:42:15,047] [   TRAIN] - Epoch=4/4, Step=100/1785 loss=0.5640 acc=0.7937 lr=0.000020 step/sec=8.78 | ETA 00:14:25\r\n",
      "[2024-08-09 06:42:16,274] [   TRAIN] - Epoch=4/4, Step=110/1785 loss=0.6533 acc=0.7750 lr=0.000020 step/sec=8.15 | ETA 00:14:25\r\n",
      "[2024-08-09 06:42:17,382] [   TRAIN] - Epoch=4/4, Step=120/1785 loss=0.6416 acc=0.8187 lr=0.000020 step/sec=9.03 | ETA 00:14:25\r\n",
      "[2024-08-09 06:42:18,483] [   TRAIN] - Epoch=4/4, Step=130/1785 loss=0.7386 acc=0.7250 lr=0.000020 step/sec=9.08 | ETA 00:14:25\r\n",
      "[2024-08-09 06:42:19,492] [   TRAIN] - Epoch=4/4, Step=140/1785 loss=0.5789 acc=0.7812 lr=0.000020 step/sec=9.91 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:20,675] [   TRAIN] - Epoch=4/4, Step=150/1785 loss=0.5559 acc=0.8125 lr=0.000020 step/sec=8.46 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:21,774] [   TRAIN] - Epoch=4/4, Step=160/1785 loss=0.6521 acc=0.7812 lr=0.000020 step/sec=9.09 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:22,928] [   TRAIN] - Epoch=4/4, Step=170/1785 loss=0.6541 acc=0.7688 lr=0.000020 step/sec=8.67 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:24,082] [   TRAIN] - Epoch=4/4, Step=180/1785 loss=0.7153 acc=0.7438 lr=0.000020 step/sec=8.66 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:25,174] [   TRAIN] - Epoch=4/4, Step=190/1785 loss=0.6701 acc=0.7312 lr=0.000020 step/sec=9.16 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:26,299] [   TRAIN] - Epoch=4/4, Step=200/1785 loss=0.6578 acc=0.7937 lr=0.000020 step/sec=8.89 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:27,414] [   TRAIN] - Epoch=4/4, Step=210/1785 loss=0.7343 acc=0.7438 lr=0.000020 step/sec=8.97 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:28,515] [   TRAIN] - Epoch=4/4, Step=220/1785 loss=0.6258 acc=0.7875 lr=0.000020 step/sec=9.08 | ETA 00:14:24\r\n",
      "[2024-08-09 06:42:29,599] [   TRAIN] - Epoch=4/4, Step=230/1785 loss=0.4731 acc=0.8812 lr=0.000020 step/sec=9.23 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:30,705] [   TRAIN] - Epoch=4/4, Step=240/1785 loss=0.5402 acc=0.8063 lr=0.000020 step/sec=9.04 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:31,849] [   TRAIN] - Epoch=4/4, Step=250/1785 loss=0.7149 acc=0.7500 lr=0.000020 step/sec=8.74 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:32,936] [   TRAIN] - Epoch=4/4, Step=260/1785 loss=0.5265 acc=0.8375 lr=0.000020 step/sec=9.20 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:34,116] [   TRAIN] - Epoch=4/4, Step=270/1785 loss=0.7305 acc=0.7562 lr=0.000020 step/sec=8.48 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:35,199] [   TRAIN] - Epoch=4/4, Step=280/1785 loss=0.8186 acc=0.6813 lr=0.000020 step/sec=9.23 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:36,388] [   TRAIN] - Epoch=4/4, Step=290/1785 loss=0.6583 acc=0.7750 lr=0.000020 step/sec=8.41 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:37,615] [   TRAIN] - Epoch=4/4, Step=300/1785 loss=0.5582 acc=0.8187 lr=0.000020 step/sec=8.15 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:38,692] [   TRAIN] - Epoch=4/4, Step=310/1785 loss=0.5138 acc=0.8500 lr=0.000020 step/sec=9.28 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:39,799] [   TRAIN] - Epoch=4/4, Step=320/1785 loss=0.7276 acc=0.7438 lr=0.000020 step/sec=9.04 | ETA 00:14:23\r\n",
      "[2024-08-09 06:42:40,906] [   TRAIN] - Epoch=4/4, Step=330/1785 loss=0.6060 acc=0.8000 lr=0.000020 step/sec=9.03 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:42,032] [   TRAIN] - Epoch=4/4, Step=340/1785 loss=0.6000 acc=0.7875 lr=0.000020 step/sec=8.88 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:43,109] [   TRAIN] - Epoch=4/4, Step=350/1785 loss=0.6215 acc=0.8000 lr=0.000020 step/sec=9.29 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:44,337] [   TRAIN] - Epoch=4/4, Step=360/1785 loss=0.6621 acc=0.7688 lr=0.000020 step/sec=8.14 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:45,414] [   TRAIN] - Epoch=4/4, Step=370/1785 loss=0.5459 acc=0.8500 lr=0.000020 step/sec=9.29 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:46,547] [   TRAIN] - Epoch=4/4, Step=380/1785 loss=0.5390 acc=0.8375 lr=0.000020 step/sec=8.83 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:47,677] [   TRAIN] - Epoch=4/4, Step=390/1785 loss=0.6948 acc=0.7375 lr=0.000020 step/sec=8.85 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:48,822] [   TRAIN] - Epoch=4/4, Step=400/1785 loss=0.6900 acc=0.7438 lr=0.000020 step/sec=8.74 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:49,916] [   TRAIN] - Epoch=4/4, Step=410/1785 loss=0.8097 acc=0.7188 lr=0.000020 step/sec=9.14 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:51,026] [   TRAIN] - Epoch=4/4, Step=420/1785 loss=0.6616 acc=0.7812 lr=0.000020 step/sec=9.01 | ETA 00:14:22\r\n",
      "[2024-08-09 06:42:52,175] [   TRAIN] - Epoch=4/4, Step=430/1785 loss=0.5739 acc=0.8063 lr=0.000020 step/sec=8.70 | ETA 00:14:21\r\n",
      "[2024-08-09 06:42:53,364] [   TRAIN] - Epoch=4/4, Step=440/1785 loss=0.4872 acc=0.8125 lr=0.000020 step/sec=8.41 | ETA 00:14:21\r\n",
      "[2024-08-09 06:42:54,458] [   TRAIN] - Epoch=4/4, Step=450/1785 loss=0.6312 acc=0.7875 lr=0.000020 step/sec=9.14 | ETA 00:14:21\r\n",
      "[2024-08-09 06:42:55,559] [   TRAIN] - Epoch=4/4, Step=460/1785 loss=0.7288 acc=0.7562 lr=0.000020 step/sec=9.08 | ETA 00:14:21\r\n",
      "[2024-08-09 06:42:56,714] [   TRAIN] - Epoch=4/4, Step=470/1785 loss=0.6781 acc=0.7750 lr=0.000020 step/sec=8.66 | ETA 00:14:21\r\n",
      "[2024-08-09 06:42:57,789] [   TRAIN] - Epoch=4/4, Step=480/1785 loss=0.5605 acc=0.7688 lr=0.000020 step/sec=9.30 | ETA 00:14:21\r\n",
      "[2024-08-09 06:42:58,885] [   TRAIN] - Epoch=4/4, Step=490/1785 loss=0.6583 acc=0.7812 lr=0.000020 step/sec=9.13 | ETA 00:14:21\r\n",
      "[2024-08-09 06:42:59,999] [   TRAIN] - Epoch=4/4, Step=500/1785 loss=0.5232 acc=0.8438 lr=0.000020 step/sec=8.97 | ETA 00:14:21\r\n",
      "[2024-08-09 06:43:01,157] [   TRAIN] - Epoch=4/4, Step=510/1785 loss=0.6298 acc=0.8063 lr=0.000020 step/sec=8.64 | ETA 00:14:21\r\n",
      "[2024-08-09 06:43:02,215] [   TRAIN] - Epoch=4/4, Step=520/1785 loss=0.6704 acc=0.7875 lr=0.000020 step/sec=9.45 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:03,312] [   TRAIN] - Epoch=4/4, Step=530/1785 loss=0.6443 acc=0.7875 lr=0.000020 step/sec=9.11 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:04,546] [   TRAIN] - Epoch=4/4, Step=540/1785 loss=0.6233 acc=0.7562 lr=0.000020 step/sec=8.11 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:05,637] [   TRAIN] - Epoch=4/4, Step=550/1785 loss=0.7273 acc=0.7188 lr=0.000020 step/sec=9.17 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:06,748] [   TRAIN] - Epoch=4/4, Step=560/1785 loss=0.7240 acc=0.7250 lr=0.000020 step/sec=9.00 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:07,925] [   TRAIN] - Epoch=4/4, Step=570/1785 loss=0.5805 acc=0.7812 lr=0.000020 step/sec=8.50 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:09,045] [   TRAIN] - Epoch=4/4, Step=580/1785 loss=0.6332 acc=0.7438 lr=0.000020 step/sec=8.93 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:10,248] [   TRAIN] - Epoch=4/4, Step=590/1785 loss=0.6835 acc=0.7750 lr=0.000020 step/sec=8.31 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:11,356] [   TRAIN] - Epoch=4/4, Step=600/1785 loss=0.5991 acc=0.8063 lr=0.000020 step/sec=9.03 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:12,536] [   TRAIN] - Epoch=4/4, Step=610/1785 loss=0.7001 acc=0.7375 lr=0.000020 step/sec=8.47 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:13,701] [   TRAIN] - Epoch=4/4, Step=620/1785 loss=0.5959 acc=0.7812 lr=0.000020 step/sec=8.58 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:14,834] [   TRAIN] - Epoch=4/4, Step=630/1785 loss=0.6093 acc=0.8250 lr=0.000020 step/sec=8.83 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:16,044] [   TRAIN] - Epoch=4/4, Step=640/1785 loss=0.5682 acc=0.7937 lr=0.000020 step/sec=8.26 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:17,137] [   TRAIN] - Epoch=4/4, Step=650/1785 loss=0.6856 acc=0.7750 lr=0.000020 step/sec=9.14 | ETA 00:14:20\r\n",
      "[2024-08-09 06:43:18,270] [   TRAIN] - Epoch=4/4, Step=660/1785 loss=0.6801 acc=0.7625 lr=0.000020 step/sec=8.83 | ETA 00:14:19\r\n",
      "[2024-08-09 06:43:19,302] [   TRAIN] - Epoch=4/4, Step=670/1785 loss=0.6252 acc=0.8000 lr=0.000020 step/sec=9.69 | ETA 00:14:19\r\n",
      "[2024-08-09 06:43:20,439] [   TRAIN] - Epoch=4/4, Step=680/1785 loss=0.6005 acc=0.7875 lr=0.000020 step/sec=8.79 | ETA 00:14:19\r\n",
      "[2024-08-09 06:43:21,568] [   TRAIN] - Epoch=4/4, Step=690/1785 loss=0.6934 acc=0.7438 lr=0.000020 step/sec=8.86 | ETA 00:14:19\r\n",
      "[2024-08-09 06:43:22,613] [   TRAIN] - Epoch=4/4, Step=700/1785 loss=0.5325 acc=0.8500 lr=0.000020 step/sec=9.57 | ETA 00:14:19\r\n",
      "[2024-08-09 06:43:23,728] [   TRAIN] - Epoch=4/4, Step=710/1785 loss=0.5100 acc=0.8313 lr=0.000020 step/sec=8.96 | ETA 00:14:19\r\n",
      "[2024-08-09 06:43:24,867] [   TRAIN] - Epoch=4/4, Step=720/1785 loss=0.6671 acc=0.7812 lr=0.000020 step/sec=8.78 | ETA 00:14:19\r\n",
      "[2024-08-09 06:43:25,909] [   TRAIN] - Epoch=4/4, Step=730/1785 loss=0.6096 acc=0.8125 lr=0.000020 step/sec=9.60 | ETA 00:14:19\r\n",
      "[2024-08-09 06:43:27,039] [   TRAIN] - Epoch=4/4, Step=740/1785 loss=0.7534 acc=0.7438 lr=0.000020 step/sec=8.84 | ETA 00:14:18\r\n",
      "[2024-08-09 06:43:28,103] [   TRAIN] - Epoch=4/4, Step=750/1785 loss=0.5446 acc=0.7937 lr=0.000020 step/sec=9.40 | ETA 00:14:18\r\n",
      "[2024-08-09 06:43:29,239] [   TRAIN] - Epoch=4/4, Step=760/1785 loss=0.6322 acc=0.8000 lr=0.000020 step/sec=8.80 | ETA 00:14:18\r\n",
      "[2024-08-09 06:43:30,368] [   TRAIN] - Epoch=4/4, Step=770/1785 loss=0.5736 acc=0.8000 lr=0.000020 step/sec=8.85 | ETA 00:14:18\r\n",
      "[2024-08-09 06:43:31,492] [   TRAIN] - Epoch=4/4, Step=780/1785 loss=0.6304 acc=0.7812 lr=0.000020 step/sec=8.90 | ETA 00:14:18\r\n",
      "[2024-08-09 06:43:32,483] [   TRAIN] - Epoch=4/4, Step=790/1785 loss=0.5019 acc=0.8562 lr=0.000020 step/sec=10.09 | ETA 00:14:18\r\n",
      "[2024-08-09 06:43:33,586] [   TRAIN] - Epoch=4/4, Step=800/1785 loss=0.6742 acc=0.7500 lr=0.000020 step/sec=9.06 | ETA 00:14:18\r\n",
      "[2024-08-09 06:43:34,600] [   TRAIN] - Epoch=4/4, Step=810/1785 loss=0.5964 acc=0.7875 lr=0.000020 step/sec=9.86 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:35,710] [   TRAIN] - Epoch=4/4, Step=820/1785 loss=0.5809 acc=0.7937 lr=0.000020 step/sec=9.02 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:36,863] [   TRAIN] - Epoch=4/4, Step=830/1785 loss=0.6132 acc=0.7812 lr=0.000020 step/sec=8.67 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:37,992] [   TRAIN] - Epoch=4/4, Step=840/1785 loss=0.5802 acc=0.7750 lr=0.000020 step/sec=8.85 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:39,035] [   TRAIN] - Epoch=4/4, Step=850/1785 loss=0.6320 acc=0.7688 lr=0.000020 step/sec=9.59 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:40,175] [   TRAIN] - Epoch=4/4, Step=860/1785 loss=0.6698 acc=0.7688 lr=0.000020 step/sec=8.77 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:41,305] [   TRAIN] - Epoch=4/4, Step=870/1785 loss=0.6028 acc=0.7812 lr=0.000020 step/sec=8.85 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:42,350] [   TRAIN] - Epoch=4/4, Step=880/1785 loss=0.6810 acc=0.7438 lr=0.000020 step/sec=9.57 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:43,478] [   TRAIN] - Epoch=4/4, Step=890/1785 loss=0.7052 acc=0.7750 lr=0.000020 step/sec=8.87 | ETA 00:14:17\r\n",
      "[2024-08-09 06:43:44,490] [   TRAIN] - Epoch=4/4, Step=900/1785 loss=0.6854 acc=0.7438 lr=0.000020 step/sec=9.88 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:45,658] [   TRAIN] - Epoch=4/4, Step=910/1785 loss=0.6843 acc=0.7438 lr=0.000020 step/sec=8.57 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:46,752] [   TRAIN] - Epoch=4/4, Step=920/1785 loss=0.6240 acc=0.8063 lr=0.000020 step/sec=9.13 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:47,889] [   TRAIN] - Epoch=4/4, Step=930/1785 loss=0.6795 acc=0.7688 lr=0.000020 step/sec=8.80 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:49,010] [   TRAIN] - Epoch=4/4, Step=940/1785 loss=0.5908 acc=0.7562 lr=0.000020 step/sec=8.91 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:50,165] [   TRAIN] - Epoch=4/4, Step=950/1785 loss=0.6483 acc=0.8000 lr=0.000020 step/sec=8.66 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:51,349] [   TRAIN] - Epoch=4/4, Step=960/1785 loss=0.6551 acc=0.7500 lr=0.000020 step/sec=8.45 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:52,470] [   TRAIN] - Epoch=4/4, Step=970/1785 loss=0.5162 acc=0.8313 lr=0.000020 step/sec=8.92 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:53,488] [   TRAIN] - Epoch=4/4, Step=980/1785 loss=0.7005 acc=0.7438 lr=0.000020 step/sec=9.82 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:54,617] [   TRAIN] - Epoch=4/4, Step=990/1785 loss=0.7154 acc=0.7750 lr=0.000020 step/sec=8.86 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:55,721] [   TRAIN] - Epoch=4/4, Step=1000/1785 loss=0.6164 acc=0.7875 lr=0.000020 step/sec=9.05 | ETA 00:14:16\r\n",
      "[2024-08-09 06:43:56,763] [   TRAIN] - Epoch=4/4, Step=1010/1785 loss=0.5778 acc=0.8063 lr=0.000020 step/sec=9.60 | ETA 00:14:15\r\n",
      "[2024-08-09 06:43:57,893] [   TRAIN] - Epoch=4/4, Step=1020/1785 loss=0.6219 acc=0.7750 lr=0.000020 step/sec=8.85 | ETA 00:14:15\r\n",
      "[2024-08-09 06:43:59,024] [   TRAIN] - Epoch=4/4, Step=1030/1785 loss=0.5826 acc=0.7875 lr=0.000020 step/sec=8.84 | ETA 00:14:15\r\n",
      "[2024-08-09 06:44:00,130] [   TRAIN] - Epoch=4/4, Step=1040/1785 loss=0.8847 acc=0.7063 lr=0.000020 step/sec=9.05 | ETA 00:14:15\r\n",
      "[2024-08-09 06:44:01,194] [   TRAIN] - Epoch=4/4, Step=1050/1785 loss=0.8173 acc=0.7312 lr=0.000020 step/sec=9.40 | ETA 00:14:15\r\n",
      "[2024-08-09 06:44:02,240] [   TRAIN] - Epoch=4/4, Step=1060/1785 loss=0.6712 acc=0.7812 lr=0.000020 step/sec=9.56 | ETA 00:14:15\r\n",
      "[2024-08-09 06:44:03,352] [   TRAIN] - Epoch=4/4, Step=1070/1785 loss=0.7442 acc=0.7312 lr=0.000020 step/sec=8.99 | ETA 00:14:15\r\n",
      "[2024-08-09 06:44:04,453] [   TRAIN] - Epoch=4/4, Step=1080/1785 loss=0.5618 acc=0.7875 lr=0.000020 step/sec=9.09 | ETA 00:14:15\r\n",
      "[2024-08-09 06:44:05,499] [   TRAIN] - Epoch=4/4, Step=1090/1785 loss=0.7129 acc=0.7500 lr=0.000020 step/sec=9.56 | ETA 00:14:14\r\n",
      "[2024-08-09 06:44:06,602] [   TRAIN] - Epoch=4/4, Step=1100/1785 loss=0.7798 acc=0.7063 lr=0.000020 step/sec=9.07 | ETA 00:14:14\r\n",
      "[2024-08-09 06:44:07,610] [   TRAIN] - Epoch=4/4, Step=1110/1785 loss=0.6512 acc=0.7625 lr=0.000020 step/sec=9.92 | ETA 00:14:14\r\n",
      "[2024-08-09 06:44:08,746] [   TRAIN] - Epoch=4/4, Step=1120/1785 loss=0.6781 acc=0.7438 lr=0.000020 step/sec=8.81 | ETA 00:14:14\r\n",
      "[2024-08-09 06:44:09,787] [   TRAIN] - Epoch=4/4, Step=1130/1785 loss=0.6164 acc=0.7750 lr=0.000020 step/sec=9.61 | ETA 00:14:14\r\n",
      "[2024-08-09 06:44:10,971] [   TRAIN] - Epoch=4/4, Step=1140/1785 loss=0.6237 acc=0.8125 lr=0.000020 step/sec=8.44 | ETA 00:14:14\r\n",
      "[2024-08-09 06:44:12,117] [   TRAIN] - Epoch=4/4, Step=1150/1785 loss=0.8484 acc=0.7375 lr=0.000020 step/sec=8.73 | ETA 00:14:14\r\n",
      "[2024-08-09 06:44:13,157] [   TRAIN] - Epoch=4/4, Step=1160/1785 loss=0.6590 acc=0.7438 lr=0.000020 step/sec=9.61 | ETA 00:14:14\r\n",
      "[2024-08-09 06:44:14,177] [   TRAIN] - Epoch=4/4, Step=1170/1785 loss=0.5864 acc=0.7812 lr=0.000020 step/sec=9.80 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:15,295] [   TRAIN] - Epoch=4/4, Step=1180/1785 loss=0.7324 acc=0.7500 lr=0.000020 step/sec=8.94 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:16,425] [   TRAIN] - Epoch=4/4, Step=1190/1785 loss=0.5362 acc=0.8375 lr=0.000020 step/sec=8.85 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:17,560] [   TRAIN] - Epoch=4/4, Step=1200/1785 loss=0.6947 acc=0.7375 lr=0.000020 step/sec=8.81 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:18,680] [   TRAIN] - Epoch=4/4, Step=1210/1785 loss=0.7731 acc=0.7500 lr=0.000020 step/sec=8.93 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:19,744] [   TRAIN] - Epoch=4/4, Step=1220/1785 loss=0.6899 acc=0.7625 lr=0.000020 step/sec=9.40 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:20,934] [   TRAIN] - Epoch=4/4, Step=1230/1785 loss=0.4894 acc=0.8313 lr=0.000020 step/sec=8.40 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:22,036] [   TRAIN] - Epoch=4/4, Step=1240/1785 loss=0.7057 acc=0.7688 lr=0.000020 step/sec=9.07 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:23,112] [   TRAIN] - Epoch=4/4, Step=1250/1785 loss=0.6408 acc=0.7688 lr=0.000020 step/sec=9.30 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:24,288] [   TRAIN] - Epoch=4/4, Step=1260/1785 loss=0.6546 acc=0.7562 lr=0.000020 step/sec=8.50 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:25,431] [   TRAIN] - Epoch=4/4, Step=1270/1785 loss=0.6035 acc=0.8063 lr=0.000020 step/sec=8.74 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:26,594] [   TRAIN] - Epoch=4/4, Step=1280/1785 loss=0.6421 acc=0.8000 lr=0.000020 step/sec=8.60 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:27,754] [   TRAIN] - Epoch=4/4, Step=1290/1785 loss=0.7463 acc=0.7063 lr=0.000020 step/sec=8.62 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:28,934] [   TRAIN] - Epoch=4/4, Step=1300/1785 loss=0.5351 acc=0.8375 lr=0.000020 step/sec=8.48 | ETA 00:14:13\r\n",
      "[2024-08-09 06:44:29,959] [   TRAIN] - Epoch=4/4, Step=1310/1785 loss=0.6595 acc=0.7875 lr=0.000020 step/sec=9.75 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:31,073] [   TRAIN] - Epoch=4/4, Step=1320/1785 loss=0.6364 acc=0.7562 lr=0.000020 step/sec=8.98 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:32,082] [   TRAIN] - Epoch=4/4, Step=1330/1785 loss=0.5598 acc=0.8000 lr=0.000020 step/sec=9.91 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:33,229] [   TRAIN] - Epoch=4/4, Step=1340/1785 loss=0.6106 acc=0.8187 lr=0.000020 step/sec=8.72 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:34,389] [   TRAIN] - Epoch=4/4, Step=1350/1785 loss=0.7051 acc=0.7812 lr=0.000020 step/sec=8.62 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:35,549] [   TRAIN] - Epoch=4/4, Step=1360/1785 loss=0.6429 acc=0.7688 lr=0.000020 step/sec=8.62 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:36,609] [   TRAIN] - Epoch=4/4, Step=1370/1785 loss=0.6206 acc=0.7812 lr=0.000020 step/sec=9.43 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:37,787] [   TRAIN] - Epoch=4/4, Step=1380/1785 loss=0.5706 acc=0.8063 lr=0.000020 step/sec=8.49 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:38,888] [   TRAIN] - Epoch=4/4, Step=1390/1785 loss=0.6655 acc=0.7812 lr=0.000020 step/sec=9.08 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:40,028] [   TRAIN] - Epoch=4/4, Step=1400/1785 loss=0.5354 acc=0.8187 lr=0.000020 step/sec=8.77 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:41,153] [   TRAIN] - Epoch=4/4, Step=1410/1785 loss=0.5432 acc=0.8438 lr=0.000020 step/sec=8.89 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:42,254] [   TRAIN] - Epoch=4/4, Step=1420/1785 loss=0.5940 acc=0.7875 lr=0.000020 step/sec=9.09 | ETA 00:14:12\r\n",
      "[2024-08-09 06:44:43,344] [   TRAIN] - Epoch=4/4, Step=1430/1785 loss=0.6778 acc=0.7438 lr=0.000020 step/sec=9.17 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:44,540] [   TRAIN] - Epoch=4/4, Step=1440/1785 loss=0.7898 acc=0.7250 lr=0.000020 step/sec=8.36 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:45,752] [   TRAIN] - Epoch=4/4, Step=1450/1785 loss=0.7848 acc=0.7000 lr=0.000020 step/sec=8.25 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:46,862] [   TRAIN] - Epoch=4/4, Step=1460/1785 loss=0.6406 acc=0.7750 lr=0.000020 step/sec=9.01 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:47,952] [   TRAIN] - Epoch=4/4, Step=1470/1785 loss=0.5563 acc=0.8000 lr=0.000020 step/sec=9.17 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:49,099] [   TRAIN] - Epoch=4/4, Step=1480/1785 loss=0.6364 acc=0.7688 lr=0.000020 step/sec=8.72 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:50,315] [   TRAIN] - Epoch=4/4, Step=1490/1785 loss=0.7866 acc=0.7063 lr=0.000020 step/sec=8.22 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:51,360] [   TRAIN] - Epoch=4/4, Step=1500/1785 loss=0.7729 acc=0.7188 lr=0.000020 step/sec=9.57 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:52,512] [   TRAIN] - Epoch=4/4, Step=1510/1785 loss=0.6404 acc=0.7500 lr=0.000020 step/sec=8.68 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:53,740] [   TRAIN] - Epoch=4/4, Step=1520/1785 loss=0.6087 acc=0.8000 lr=0.000020 step/sec=8.14 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:54,850] [   TRAIN] - Epoch=4/4, Step=1530/1785 loss=0.7513 acc=0.7688 lr=0.000020 step/sec=9.01 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:55,954] [   TRAIN] - Epoch=4/4, Step=1540/1785 loss=0.7078 acc=0.7625 lr=0.000020 step/sec=9.06 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:57,055] [   TRAIN] - Epoch=4/4, Step=1550/1785 loss=0.6392 acc=0.7812 lr=0.000020 step/sec=9.08 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:58,153] [   TRAIN] - Epoch=4/4, Step=1560/1785 loss=0.6129 acc=0.7688 lr=0.000020 step/sec=9.11 | ETA 00:14:11\r\n",
      "[2024-08-09 06:44:59,310] [   TRAIN] - Epoch=4/4, Step=1570/1785 loss=0.7418 acc=0.7250 lr=0.000020 step/sec=8.64 | ETA 00:14:11\r\n",
      "[2024-08-09 06:45:00,498] [   TRAIN] - Epoch=4/4, Step=1580/1785 loss=0.7429 acc=0.7500 lr=0.000020 step/sec=8.42 | ETA 00:14:11\r\n",
      "[2024-08-09 06:45:01,590] [   TRAIN] - Epoch=4/4, Step=1590/1785 loss=0.7441 acc=0.7438 lr=0.000020 step/sec=9.16 | ETA 00:14:11\r\n",
      "[2024-08-09 06:45:02,720] [   TRAIN] - Epoch=4/4, Step=1600/1785 loss=0.5897 acc=0.8187 lr=0.000020 step/sec=8.85 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:03,776] [   TRAIN] - Epoch=4/4, Step=1610/1785 loss=0.5562 acc=0.8250 lr=0.000020 step/sec=9.47 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:04,888] [   TRAIN] - Epoch=4/4, Step=1620/1785 loss=0.8711 acc=0.6875 lr=0.000020 step/sec=8.99 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:06,033] [   TRAIN] - Epoch=4/4, Step=1630/1785 loss=0.6622 acc=0.7937 lr=0.000020 step/sec=8.73 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:07,233] [   TRAIN] - Epoch=4/4, Step=1640/1785 loss=0.7064 acc=0.7937 lr=0.000020 step/sec=8.34 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:08,360] [   TRAIN] - Epoch=4/4, Step=1650/1785 loss=0.6901 acc=0.7500 lr=0.000020 step/sec=8.87 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:09,504] [   TRAIN] - Epoch=4/4, Step=1660/1785 loss=0.6350 acc=0.7750 lr=0.000020 step/sec=8.75 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:10,622] [   TRAIN] - Epoch=4/4, Step=1670/1785 loss=0.6297 acc=0.7750 lr=0.000020 step/sec=8.94 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:11,776] [   TRAIN] - Epoch=4/4, Step=1680/1785 loss=0.5180 acc=0.7875 lr=0.000020 step/sec=8.67 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:12,909] [   TRAIN] - Epoch=4/4, Step=1690/1785 loss=0.7975 acc=0.7188 lr=0.000020 step/sec=8.83 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:14,058] [   TRAIN] - Epoch=4/4, Step=1700/1785 loss=0.6152 acc=0.7937 lr=0.000020 step/sec=8.71 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:15,154] [   TRAIN] - Epoch=4/4, Step=1710/1785 loss=0.6967 acc=0.8187 lr=0.000020 step/sec=9.12 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:16,292] [   TRAIN] - Epoch=4/4, Step=1720/1785 loss=0.7078 acc=0.7625 lr=0.000020 step/sec=8.79 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:17,403] [   TRAIN] - Epoch=4/4, Step=1730/1785 loss=0.5221 acc=0.8187 lr=0.000020 step/sec=9.00 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:18,502] [   TRAIN] - Epoch=4/4, Step=1740/1785 loss=0.6100 acc=0.7937 lr=0.000020 step/sec=9.10 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:19,711] [   TRAIN] - Epoch=4/4, Step=1750/1785 loss=0.6626 acc=0.7750 lr=0.000020 step/sec=8.27 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:20,870] [   TRAIN] - Epoch=4/4, Step=1760/1785 loss=0.5481 acc=0.8000 lr=0.000020 step/sec=8.63 | ETA 00:14:10\r\n",
      "[2024-08-09 06:45:21,987] [   TRAIN] - Epoch=4/4, Step=1770/1785 loss=0.6335 acc=0.7562 lr=0.000020 step/sec=8.95 | ETA 00:14:09\r\n",
      "[2024-08-09 06:45:23,087] [   TRAIN] - Epoch=4/4, Step=1780/1785 loss=0.6172 acc=0.7812 lr=0.000020 step/sec=9.09 | ETA 00:14:09\r\n",
      "[2024-08-09 06:45:32,321] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation \r\n",
      "[2024-08-09 06:45:32,325] [    INFO] - Saving model checkpoint to ./ckpt/epoch_4\r\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataset, epochs=4, batch_size=16, eval_dataset=dev_dataset, save_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:46:00.407869Z",
     "iopub.status.busy": "2024-08-08T22:46:00.407218Z",
     "iopub.status.idle": "2024-08-08T22:46:09.268280Z",
     "shell.execute_reply": "2024-08-08T22:46:09.267149Z",
     "shell.execute_reply.started": "2024-08-08T22:46:00.407822Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-09 06:46:09,262] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation \r\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:48:21.654742Z",
     "iopub.status.busy": "2024-08-08T22:48:21.653844Z",
     "iopub.status.idle": "2024-08-08T22:48:30.709036Z",
     "shell.execute_reply": "2024-08-08T22:48:30.708200Z",
     "shell.execute_reply.started": "2024-08-08T22:48:21.654674Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4906447820373629 0.5149515440398932 0.47754272034650835\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = pd.read_csv('/home/aistudio/data/data100731/test.csv',sep = '\\t')\n",
    "news1 = pd.DataFrame(columns=['label'])\n",
    "news1['label'] = df[\"label\"]\n",
    "news = pd.DataFrame(columns=['text_a'])\n",
    "news['text_a'] = df[\"text_a\"]\n",
    "data_array = np.array(news)\n",
    "data_list =data_array.tolist()\n",
    "y_pre = model.predict(data_list, max_seq_len=128, batch_size=16, use_gpu=True)\n",
    "data_array1 = np.array(news1)\n",
    "y_val =data_array1.tolist()\n",
    "from sklearn.metrics import precision_recall_fscore_support,f1_score,precision_score,recall_score\n",
    "f1 = f1_score(y_val, y_pre, average='macro')\n",
    "p = precision_score(y_val, y_pre, average='macro')\n",
    "r = recall_score(y_val, y_pre, average='macro')\n",
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T22:48:39.932707Z",
     "iopub.status.busy": "2024-08-08T22:48:39.932056Z",
     "iopub.status.idle": "2024-08-08T22:48:51.732333Z",
     "shell.execute_reply": "2024-08-08T22:48:51.731404Z",
     "shell.execute_reply.started": "2024-08-08T22:48:39.932666Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-09 06:48:39,941] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-tiny/ernie_tiny.pdparams\r\n",
      "[2024-08-09 06:48:45,925] [    INFO] - Loaded parameters from /home/aistudio/data/data100731/ckpt/best_model/model.pdparams\r\n",
      "[2024-08-09 06:48:45,934] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/vocab.txt\r\n",
      "[2024-08-09 06:48:45,937] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/spm_cased_simp_sampled.model\r\n",
      "[2024-08-09 06:48:45,939] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/dict.wordseg.pickle\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 真的是惊天逆转！日本男团2-3不敌瑞典，张本智和情绪崩溃，整个人看起来要碎掉了，赛后称自己铜... \t Lable: 惊讶\r\n",
      "Data: 心态需要加强，太想赢的人总是会输，专注比赛吧，坦然接受结果，你可以失败 \t Lable: 难过\r\n",
      "Data: 三项两项已经没有奖牌了现在团体也只能争铜牌，要我我也崩溃 \t Lable: 难过\r\n",
      "Data: 体育竞技好残酷 \t Lable: 难过\r\n",
      "Data: 感觉他们队就他一个人特别努力，太累了 \t Lable: 难过\r\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "  [\"真的是惊天逆转！日本男团2-3不敌瑞典，张本智和情绪崩溃，整个人看起来要碎掉了，赛后称自己铜...\"],\n",
    "  [\"心态需要加强，太想赢的人总是会输，专注比赛吧，坦然接受结果，你可以失败\"],\n",
    "  [\"三项两项已经没有奖牌了现在团体也只能争铜牌，要我我也崩溃\"],\n",
    "  [\"体育竞技好残酷\"],\n",
    "  [\"感觉他们队就他一个人特别努力，太累了\"]\n",
    "]\n",
    "\n",
    "# 定义要进行情感分类的7个类别\n",
    "label_list=['难过', '愉快', '喜欢', '愤怒', '害怕', '惊讶', '厌恶']\n",
    "label_map = {\n",
    "    idx: label_text for idx, label_text in enumerate(label_list)\n",
    "}\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = hub.Module(\n",
    "    name='ernie_tiny',\n",
    "    task='seq-cls',\n",
    "    num_classes=7,\n",
    "    load_checkpoint='./ckpt/best_model/model.pdparams',\n",
    "    label_map=label_map)\n",
    "\n",
    "# 进行模型预测\n",
    "results = model.predict(data, max_seq_len=128, batch_size=1, use_gpu=True)\n",
    "for idx, text in enumerate(data):\n",
    "    print('Data: {} \\t Lable: {}'.format(text[0], results[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T23:05:48.319486Z",
     "iopub.status.busy": "2024-08-08T23:05:48.318882Z",
     "iopub.status.idle": "2024-08-08T23:06:08.470886Z",
     "shell.execute_reply": "2024-08-08T23:06:08.470038Z",
     "shell.execute_reply.started": "2024-08-08T23:05:48.319433Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据预览:\r\n",
      "[['#张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 \\u200b'], ['#张本智和 甄嬛# 乒乓版的马琳，打球喜欢乱叫，体力都消耗掉了还能安心比赛吗 \\u200b'], ['#张本智和 甄嬛# 不要太搞笑，什么都和甄嬛挂边 \\u200b'], ['#张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 \\u200b'], ['#张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 \\u200b']]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-09 07:05:48,344] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-tiny/ernie_tiny.pdparams\r\n",
      "[2024-08-09 07:05:54,466] [    INFO] - Loaded parameters from /home/aistudio/data/data100731/ckpt/best_model/model.pdparams\r\n",
      "[2024-08-09 07:05:54,778] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/vocab.txt\r\n",
      "[2024-08-09 07:05:54,782] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/spm_cased_simp_sampled.model\r\n",
      "[2024-08-09 07:05:54,786] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-tiny/dict.wordseg.pickle\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: #张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 乒乓版的马琳，打球喜欢乱叫，体力都消耗掉了还能安心比赛吗 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 不要太搞笑，什么都和甄嬛挂边 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了， ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了，一个自愿加入日本籍的中国人，到头来还有国人的热血？就像麻婆豆腐里放草莓一样令人作呕。对于一个日本人，永远都别同情，更别心疼。 收起 d \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于 ​ 展开 c \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于🐲至今未得到应当属于他的最大祝福…这群人真是换汤不换药 收起 d \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一个正当比赛 努力 谦逊的运动员 都应该被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 乒乓版的马琳，打球喜欢乱叫，体力都消耗掉了还能安心比赛吗 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 不要太搞笑，什么都和甄嬛挂边 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了， ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了，一个自愿加入日本籍的中国人，到头来还有国人的热血？就像麻婆豆腐里放草莓一样令人作呕。对于一个日本人，永远都别同情，更别心疼。 收起 d \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于 ​ 展开 c \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于🐲至今未得到应当属于他的最大祝福…这群人真是换汤不换药 收起 d \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一个正当比赛 努力 谦逊的运动员 都应该被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 不要太搞笑，什么都和甄嬛挂边 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了， ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了，一个自愿加入日本籍的中国人，到头来还有国人的热血？就像麻婆豆腐里放草莓一样令人作呕。对于一个日本人，永远都别同情，更别心疼。 收起 d \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于 ​ 展开 c \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于🐲至今未得到应当属于他的最大祝福…这群人真是换汤不换药 收起 d \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一个正当比赛 努力 谦逊的运动员 都应该被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 虽然很可怜咋有点好笑呢 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 哈哈哈哈哈哈哈哈哈，笑死我了，今天都是搞笑热搜么？ ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了， ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了，一个自愿加入日本籍的中国人，到头来还有国人的热血？就像麻婆豆腐里放草莓一样令人作呕。对于一个日本人，永远都别同情，更别心疼。 收起 d \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于 ​ 展开 c \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于🐲至今未得到应当属于他的最大祝福…这群人真是换汤不换药 收起 d \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一个正当比赛 努力 谦逊的运动员 都应该被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 虽然很可怜咋有点好笑呢 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 哈哈哈哈哈哈哈哈哈，笑死我了，今天都是搞笑热搜么？ ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了， ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了，一个自愿加入日本籍的中国人，到头来还有国人的热血？就像麻婆豆腐里放草莓一样令人作呕。对于一个日本人，永远都别同情，更别心疼。 收起 d \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于 ​ 展开 c \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于🐲至今未得到应当属于他的最大祝福…这群人真是换汤不换药 收起 d \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一个正当比赛 努力 谦逊的运动员 都应该被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 哈哈哈哈哈哈哈哈哈，笑死我了，今天都是搞笑热搜么？ ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了， ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了，一个自愿加入日本籍的中国人，到头来还有国人的热血？就像麻婆豆腐里放草莓一样令人作呕。对于一个日本人，永远都别同情，更别心疼。 收起 d \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于 ​ 展开 c \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于🐲至今未得到应当属于他的最大祝福…这群人真是换汤不换药 收起 d \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一个正当比赛 努力 谦逊的运动员 都应该被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 哈哈哈哈哈哈哈哈哈，笑死我了，今天都是搞笑热搜么？ ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了， ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了，一个自愿加入日本籍的中国人，到头来还有国人的热血？就像麻婆豆腐里放草莓一样令人作呕。对于一个日本人，永远都别同情，更别心疼。 收起 d \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 可怜的纸盒，实力挺好的但是运气不好，这次一个奖牌都没拿到应该得介怀很久 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 哈哈哈哈哈哈哈哈哈，笑死我了，今天都是搞笑热搜么？ ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 可怜的纸盒，实力挺好的但是运气不好，这次一个奖牌都没拿到应该得介怀很久 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 可怜的纸盒，实力挺好的但是运气不好，这次一个奖牌都没拿到应该得介怀很久 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 可怜的纸盒，实力挺好的但是运气不好，这次一个奖牌都没拿到应该得介怀很久 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 怜爱一下下，非常努力但是得不到回报甚至队友还笑呵呵的对待这场失败真的有点点心疼了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: 张本智和真的是面如死灰…有些时候觉得日本人胜负欲确实是极强的，而且这种失败之后要死的感觉就和他们战败要切腹的名字精神一样… #张本智和 甄嬛# ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 我屏蔽这货了 ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 搞不懂了为什么对一个努力的运动员这么大恶意？体育无国界呀。。这样歧视真的好吗。。我看纸盒脱口而出中文想必家里也没有让他忘本。他又不是做了什么十恶不赦的事情网友们嘴下留情吧 纸盒先是一位国家运动员再是一位日本人。。 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 笑死我了 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 没事哒没事哒 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# 比赛输了  小本儿一脸的生无可恋  话说本本儿身上还是有一点点破碎感的  这种碎了的感觉好演员都演不出来  他是真的碎了 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# yx太过了，反感！小日子滚蛋！ ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 体育无国界，人有！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 搞不懂了为什么对一个努力的运动员这么大恶意？体育无国界呀。。这样歧视真的好吗。。我看纸盒脱口而出中文想必家里也没有让他忘本。他又不是做了什么十恶不赦的事情网友们嘴下留情吧 纸盒先是一位国家运动员再是一位日本人。。 ​ \t Lable: 厌恶\r\n",
      "Data: 张本智和真的是面如死灰…有些时候觉得日本人胜负欲确实是极强的，而且这种失败之后要死的感觉就和他们战败要切腹的名字精神一样… #张本智和 甄嬛# ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 他真的太可怜了，实在不行给他一块keep奖牌安抚一下呢 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# yx太过了，反感！小日子滚蛋！ ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 体育无国界，人有！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 搞不懂了为什么对一个努力的运动员这么大恶意？体育无国界呀。。这样歧视真的好吗。。我看纸盒脱口而出中文想必家里也没有让他忘本。他又不是做了什么十恶不赦的事情网友们嘴下留情吧 纸盒先是一位国家运动员再是一位日本人。。 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和 甄嬛# 妈的给他一块keep奖牌也行啊。 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连 ​ 展开 c \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 其实咱也不是一点同情心都没有   毕竟咱也不是那种落井下石的人  可是谁让他赛前那样的信誓旦旦呢  啊啊  一定会为小日子带回3块奖牌  这才2：0 呢  10：0我都不会激动  笑死人了  我说小本本儿  看在你初出茅庐的份儿上  告诉你一句中国古话  人外有人天外有天  强中自有强中手  就连国乒那么强  也没有一个人敢这么藐视对手的  赛前早早的就放出大话来吓唬人  到时候只怕耳瓜子会比奖牌来得更快些   打得更响亮 #张本智和曾说要带3枚奖牌回日本# 收起 d \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 圣母和小日本🔒！gun！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 账本纸盒-乒坛安陵容 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 他真的太可怜了，实在不行给他一块keep奖牌安抚一下呢 ​ \t Lable: 愉快\r\n",
      "Data: #张本智和 甄嬛# yx太过了，反感！小日子滚蛋！ ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 体育无国界，人有！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 搞不懂了为什么对一个努力的运动员这么大恶意？体育无国界呀。。这样歧视真的好吗。。我看纸盒脱口而出中文想必家里也没有让他忘本。他又不是做了什么十恶不赦的事情网友们嘴下留情吧 纸盒先是一位国家运动员再是一位日本人。。 ​ \t Lable: 厌恶\r\n",
      "Data: #张本智和 甄嬛# 尊重运动员？日本的运动员？不是，你们中邪了？别的国家还好说，同情尊重日本的运动员，是不是太圣母了？！ ​ \t Lable: 愤怒\r\n",
      "Data: #张本智和 甄嬛# 一开始觉得中国人转日本队，当代汉奸，可恨。现在只觉得这货真可怜🤷🏻‍♀️他都伤心麻了，队友咧着大牙乐呢 ​ \t Lable: 难过\r\n",
      "Data: #张本智和 甄嬛# 每一位拼尽全力的选手都值得被尊重 ​ \t Lable: 喜欢\r\n",
      "Data: #张本智和甄嬛# 每个积极努力认真的运动员都该被尊重！ ​ \t Lable: 喜欢\r\n",
      "[['#张本智和 甄嬛# 每一个认真真诚友善的参加比赛的人都应该被尊重 无论国籍 \\u200b'], ['#张本智和 甄嬛# 乒乓版的马琳，打球喜欢乱叫，体力都消耗掉了还能安心比赛吗 \\u200b'], ['#张本智和 甄嬛# 不要太搞笑，什么都和甄嬛挂边 \\u200b'], ['#张本智和 甄嬛# “如果不踩着你的梦想上去，我的梦想就碎了” L 双门一棵小趴菜的微博视频 \\u200b'], ['#张本智和 甄嬛# 还是别和运动员开这种玩笑。尊重一下运动员吧 \\u200b'], ['#张本智和 甄嬛# 我靠哈哈哈哈哈哈哈哈 \\u200b'], ['#张本智和 甄嬛# 这么看的话本子真的好惨，摊上这二个垮批队友，张本破碎感拉满 \\u200b'], ['#张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了， \\u200b 展开 c'], ['#张本智和 甄嬛# 别了，张本智和配不上甄嬛的头衔，甄嬛最后还会赢到最后，你们都希望张本智和跟他的国家也成为最后的赢家吗？一个小日本，同情心都那么强，拜托看看他之前的言论吧，真下头，别说什么因为他父母，他父母能让孩子入日本籍也不是好东西。说他骨子里还有国人的热血，这样一说就更恶心了，一个自愿加入日本籍的中国人，到头来还有国人的热血？就像麻婆豆腐里放草莓一样令人作呕。对于一个日本人，永远都别同情，更别心疼。 收起 d'], ['#张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种挑谌龙的刺以至于 \\u200b 展开 c'], ['#张本智和 甄嬛# 能不能别yx了，这人身心乃至姓名都归化了，且现在的目标是重点研究怎么击败中国队，看樊和他的那场比赛樊都累成什么样了…为什么打他比打任何人的压力都大各位不清楚吗…不经联想到之前羽球谌龙顶着巨大压力守住男单冠军却因为一群自我代入感极强的圣人心疼李导致各种\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import paddlehub as hub\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "# 读取 Excel 文件\r\n",
    "df = pd.read_excel('/home/aistudio/weibo_nlp_comments_combined.xlsx')\r\n",
    "\r\n",
    "# 提取 'Comment' 列的数据，转换为符合模型要求的格式\r\n",
    "data = df['Comment'].tolist()\r\n",
    "data = [[comment] for comment in data]  # 将每条评论包装在一个列表中\r\n",
    "\r\n",
    "# 打印数据的前几条以确认格式\r\n",
    "print(\"数据预览:\")\r\n",
    "print(data[:5])\r\n",
    "\r\n",
    "# 定义要进行情感分类的7个类别\r\n",
    "label_list = ['难过', '愉快', '喜欢', '愤怒', '害怕', '惊讶', '厌恶']\r\n",
    "label_map = {\r\n",
    "    idx: label_text for idx, label_text in enumerate(label_list)\r\n",
    "}\r\n",
    "reverse_label_map = {idx: label for idx, label in label_map.items()}\r\n",
    "# 加载训练好的模型\r\n",
    "model = hub.Module(\r\n",
    "    name='ernie_tiny',\r\n",
    "    task='seq-cls',\r\n",
    "    num_classes=7,\r\n",
    "    load_checkpoint='./ckpt/best_model/model.pdparams',\r\n",
    "    label_map=label_map\r\n",
    ")\r\n",
    "results = model.predict(data, max_seq_len=128, batch_size=1, use_gpu=True)\r\n",
    "for idx, text in enumerate(data):\r\n",
    "    print('Data: {} \\t Lable: {}'.format(text[0], results[idx]))\r\n",
    "\r\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T23:26:41.714905Z",
     "iopub.status.busy": "2024-08-08T23:26:41.714420Z",
     "iopub.status.idle": "2024-08-08T23:26:42.703843Z",
     "shell.execute_reply": "2024-08-08T23:26:42.702947Z",
     "shell.execute_reply.started": "2024-08-08T23:26:41.714867Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness      99\r\n",
      "anger        85\r\n",
      "like         70\r\n",
      "happiness    55\r\n",
      "disgust      13\r\n",
      "Name: label, dtype: int64\r\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "final = pd.DataFrame(columns=['comment', 'label'])\r\n",
    "for idx, text in enumerate(data):\r\n",
    "    final = final.append({'comment': text[0], 'label': results[idx]}, ignore_index=True)\r\n",
    "label_counts = final['label'].value_counts().reset_index()\r\n",
    "label_counts.columns = ['label', 'count']\r\n",
    "\r\n",
    "label_mapping1 = {\r\n",
    " '难过':'sadness',\r\n",
    "'愉快':'happiness',\r\n",
    " '喜欢':'like',\r\n",
    " '愤怒':'anger',\r\n",
    "'害怕':'fear',\r\n",
    " '惊讶':'surprise',\r\n",
    "'厌恶':'disgust'\r\n",
    "}\r\n",
    "final['label'] = final['label'].map(label_mapping1)\r\n",
    "label_counts = final['label'].value_counts()\r\n",
    "print(label_counts)\r\n",
    "plt.figure(figsize=(10, 6))\r\n",
    "label_counts.plot(kind='bar', color='skyblue')\r\n",
    "plt.title('Distribution of Labels')\r\n",
    "plt.xlabel('Label')\r\n",
    "plt.ylabel('Count')\r\n",
    "plt.xticks(rotation=45)\r\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
